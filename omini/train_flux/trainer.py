import lightning as L
from diffusers.pipelines import FluxPipeline
import torch
import wandb
import os
import yaml
from peft import LoraConfig, get_peft_model_state_dict, set_peft_model_state_dict
from torch.utils.data import DataLoader
from safetensors.torch import load_file,save_file
import time
from torch.distributions import Normal
from diffusers.models.embeddings import (
    Timesteps,          # æ—¶é—´æ­¥åŸºç¡€ç¼–ç ï¼ˆsin/cos ä½ç½®ç¼–ç ï¼‰
    TimestepEmbedding,# æ—¶é—´æ­¥åµŒå…¥å±‚ï¼ˆMLP æ˜ å°„ï¼‰
    PixArtAlphaTextProjection
)
from PIL import Image, ImageFilter
from typing import List, Optional, Dict, Tuple
import torch.nn as nn
import prodigyopt
from accelerate import load_checkpoint_and_dispatch

from ..pipeline.flux_omini import transformer_forward, encode_images, Condition


# LOCAL_FLUX_DIR = "/FLUX.1-dev"
def get_rank():
    try:
        rank = int(os.environ.get("LOCAL_RANK"))
    except:
        rank = 0
    return rank


def get_config():
    config_path = os.environ.get("OMINI_CONFIG")
    assert config_path is not None, "Please set the OMINI_CONFIG environment variable"
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    return config


def init_wandb(wandb_config, run_name):
    import wandb

    try:
        assert os.environ.get("WANDB_API_KEY") is not None
        wandb.init(
            project=wandb_config["project"],
            name=run_name,
            config={},
        )
    except Exception as e:
        print("Failed to initialize WanDB:", e)
class time_text_embed_module2(nn.Module):
    def __init__(self, embedding_dim, pooled_projection_dim):
        super().__init__()

        # 1. æ—¶é—´æ­¥ t çš„åŸºç¡€ç¼–ç ï¼ˆä¿æŒä¸å˜ï¼‰
        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)

        # 2. æ—¶é—´é—´éš” Î”t çš„åŸºç¡€ç¼–ç ï¼ˆä¿æŒä¸å˜ï¼Œä¸ t ç»“æ„ä¸€è‡´ï¼‰
        self.delta_time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.delta_timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)

        # 5. åŸæœ‰æ–‡æœ¬ç‰¹å¾ç¼–ç ï¼ˆä¿æŒä¸å˜ï¼‰
        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn="silu")

    def forward(self, timestep, delta_timestep, pooled_projection):
        """
        è¾“å…¥æ‰©å±•ï¼šæ–°å¢ delta_timestepï¼ˆÎ”t = t - rï¼‰
        args:
            timestep: æ—¶é—´æ­¥ tï¼ˆshape: (batch_size,)ï¼‰
            delta_timestep: æ—¶é—´é—´éš” Î”t = t - rï¼ˆshape: (batch_size,)ï¼‰
            pooled_projection: æ–‡æœ¬æ± åŒ–ç‰¹å¾ï¼ˆshape: (batch_size, pooled_projection_dim)ï¼‰
        return:
            conditioning: èåˆ (t, Î”t, æ–‡æœ¬) çš„æœ€ç»ˆåµŒå…¥ï¼ˆshape: (batch_size, embedding_dim)ï¼‰
        """
        # æ­¥éª¤1ï¼šå¯¹ t è¿›è¡Œä½ç½®ç¼–ç ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
        t_proj = self.time_proj(timestep)
        t_emb = self.timestep_embedder(t_proj.to(dtype=pooled_projection.dtype))  # (B, embedding_dim)

        # æ­¥éª¤2ï¼šå¯¹ Î”t è¿›è¡Œä½ç½®ç¼–ç ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
        delta_t_proj = self.delta_time_proj(delta_timestep)
        delta_t_emb = self.delta_timestep_embedder(delta_t_proj.to(dtype=pooled_projection.dtype))  # (B, embedding_dim)

        # ä¿®æ”¹åçš„ä»£ç ï¼šå¼ºåˆ¶å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹å±‚æ‰€åœ¨çš„è®¾å¤‡
        target_device = self.text_embedder.linear_1.weight.device  # è·å–æ¨¡å‹å±‚çš„è®¾å¤‡(é€šå¸¸æ˜¯cuda:0)
        text_emb = self.text_embedder(pooled_projection.to(target_device))
        # æ­¥éª¤6ï¼šæ–‡æœ¬ç‰¹å¾ç¼–ç ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
        # text_emb = self.text_embedder(pooled_projection)  # (B, embedding_dim)

        # æ­¥éª¤7ï¼šèåˆæ—¶é—´ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ï¼ˆä¿æŒåŸæœ‰åŠ æ³•èåˆé€»è¾‘ï¼‰
        conditioning = t_emb + delta_t_emb + text_emb

        return conditioning
class time_text_embed_module1(nn.Module):
    def __init__(self, embedding_dim, pooled_projection_dim):
        super().__init__()

        # 1. æ—¶é—´æ­¥ t çš„åŸºç¡€ç¼–ç ï¼ˆä¿æŒä¸å˜ï¼‰
        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)


        # 3. ä¸º t å•ç‹¬åˆ›å»º 2-layer MLPï¼ˆç»“æ„ä¸åŸå…±äº«MLPä¸€è‡´ï¼‰
        self.timestep_mlp = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),  # è¾“å…¥ç»´åº¦ä¸º embedding_dimï¼ˆå•ä¸€æ—¶é—´ç¼–ç ç»´åº¦ï¼‰
            nn.SiLU(),  # æ¿€æ´»å‡½æ•°ä¿æŒä¸åŸä»£ç ä¸€è‡´
            nn.Linear(embedding_dim, embedding_dim)
        )

        # 4. ä¸º Î”t å•ç‹¬åˆ›å»º 2-layer MLPï¼ˆä¸ t çš„MLPç»“æ„å®Œå…¨ä¸€è‡´ï¼Œä¿è¯å¯¹ç­‰å¤„ç†ï¼‰
        self.delta_timestep_mlp = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.SiLU(),
            nn.Linear(embedding_dim, embedding_dim)
        )

        # 5. åŸæœ‰æ–‡æœ¬ç‰¹å¾ç¼–ç ï¼ˆä¿æŒä¸å˜ï¼‰
        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn="silu")

    def forward(self, timestep, delta_timestep, pooled_projection):
        """
        è¾“å…¥æ‰©å±•ï¼šæ–°å¢ delta_timestepï¼ˆÎ”t = t - rï¼‰
        args:
            timestep: æ—¶é—´æ­¥ tï¼ˆshape: (batch_size,)ï¼‰
            delta_timestep: æ—¶é—´é—´éš” Î”t = t - rï¼ˆshape: (batch_size,)ï¼‰
            pooled_projection: æ–‡æœ¬æ± åŒ–ç‰¹å¾ï¼ˆshape: (batch_size, pooled_projection_dim)ï¼‰
        return:
            conditioning: èåˆ (t, Î”t, æ–‡æœ¬) çš„æœ€ç»ˆåµŒå…¥ï¼ˆshape: (batch_size, embedding_dim)ï¼‰
        """
        # æ­¥éª¤1ï¼šå¯¹ t è¿›è¡Œä½ç½®ç¼–ç ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
        t_proj = self.time_proj(timestep)
        t_emb = self.timestep_embedder(t_proj.to(dtype=pooled_projection.dtype))  # (B, embedding_dim)

        # æ­¥éª¤2ï¼šå¯¹ Î”t è¿›è¡Œä½ç½®ç¼–ç ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
        delta_t_proj = self.time_proj(delta_timestep)
        delta_t_emb = self.timestep_embedder(delta_t_proj.to(dtype=pooled_projection.dtype))  # (B, embedding_dim)

        # æ­¥éª¤3ï¼št ç»è¿‡ç‹¬ç«‹çš„ 2-layer MLP å¤„ç†
        t_emb_processed = self.timestep_mlp(t_emb)  # (B, embedding_dim)

        # æ­¥éª¤4ï¼šÎ”t ç»è¿‡ç‹¬ç«‹çš„ 2-layer MLP å¤„ç†
        delta_t_emb_processed = self.delta_timestep_mlp(delta_t_emb)  # (B, embedding_dim)

        # æ­¥éª¤5ï¼šæ±‚å’Œèåˆ t å’Œ Î”t çš„å¤„ç†ç»“æœï¼ˆç¬¦åˆä½ çš„è¦æ±‚ï¼‰
        fused_time_emb = t_emb_processed + delta_t_emb_processed  # (B, embedding_dim)

        # æ­¥éª¤6ï¼šæ–‡æœ¬ç‰¹å¾ç¼–ç ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
        text_emb = self.text_embedder(pooled_projection)  # (B, embedding_dim)

        # æ­¥éª¤7ï¼šèåˆæ—¶é—´ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ï¼ˆä¿æŒåŸæœ‰åŠ æ³•èåˆé€»è¾‘ï¼‰
        conditioning = fused_time_emb + text_emb

        return conditioning
class time_text_embed_module(nn.Module):
    def __init__(self, embedding_dim, pooled_projection_dim):
        super().__init__()

        # 1. åŸæœ‰æ—¶é—´æ­¥åŸºç¡€ç¼–ç ï¼ˆä¿æŒä¸å˜ï¼‰
        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)

        # 2. æ–°å¢ delta_tï¼ˆÎ”t = t - rï¼‰çš„åŸºç¡€ç¼–ç ï¼ˆä¸ t ç»“æ„ä¸€è‡´ï¼‰
        self.delta_time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.delta_timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)

        # 3. èåˆ t å’Œ delta_t ç¼–ç çš„ 2-layer MLPï¼ˆè®ºæ–‡ 4.3 è¦æ±‚ï¼‰
        self.time_fusion_mlp = nn.Sequential(
            nn.Linear(2 * embedding_dim, embedding_dim),
            nn.SiLU(),  # æ¿€æ´»å‡½æ•°ä¸æ–‡æœ¬åµŒå…¥ä¿æŒä¸€è‡´
            nn.Linear(embedding_dim, embedding_dim)
        )

        # 4. åŸæœ‰æ–‡æœ¬ç‰¹å¾ç¼–ç ï¼ˆä¿æŒä¸å˜ï¼‰
        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn="silu")

    def forward(self, timestep, delta_timestep, pooled_projection):
        """
        è¾“å…¥æ‰©å±•ï¼šæ–°å¢ delta_timestepï¼ˆÎ”t = t - rï¼‰
        args:
            timestep: æ—¶é—´æ­¥ tï¼ˆshape: (batch_size,)ï¼‰
            delta_timestep: æ—¶é—´é—´éš” Î”t = t - rï¼ˆshape: (batch_size,)ï¼‰
            pooled_projection: æ–‡æœ¬æ± åŒ–ç‰¹å¾ï¼ˆshape: (batch_size, pooled_projection_dim)ï¼‰
        return:
            conditioning: èåˆ (t, Î”t, æ–‡æœ¬) çš„æœ€ç»ˆåµŒå…¥ï¼ˆshape: (batch_size, embedding_dim)ï¼‰
        """
        # æ­¥éª¤1ï¼šå¯¹ t è¿›è¡Œç¼–ç ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        t_proj = self.time_proj(timestep)
        t_emb = self.timestep_embedder(t_proj.to(dtype=pooled_projection.dtype))  # (B, pos_embed_dim)

        # æ­¥éª¤2ï¼šå¯¹ Î”t è¿›è¡Œç¼–ç ï¼ˆæ–°å¢é€»è¾‘ï¼Œä¸ t ç¼–ç ç»“æ„å®Œå…¨ä¸€è‡´ï¼‰
        delta_t_proj = self.delta_time_proj(delta_timestep)
        delta_t_emb = self.delta_timestep_embedder(delta_t_proj.to(dtype=pooled_projection.dtype))  # (B, pos_embed_dim)

        # æ­¥éª¤3ï¼šèåˆ t å’Œ Î”t çš„ç¼–ç ï¼ˆè®ºæ–‡æ ¸å¿ƒè¦æ±‚ï¼šuÎ¸(Â·, r, t) â‰œ net(Â·, t, tâˆ’r)ï¼‰
        combined_time_emb = torch.cat([t_emb, delta_t_emb], dim=-1)  # (B, 2*pos_embed_dim)
        fused_time_emb = self.time_fusion_mlp(combined_time_emb)  # (B, embedding_dim)

        # æ­¥éª¤4ï¼šæ–‡æœ¬ç‰¹å¾ç¼–ç ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
        text_emb = self.text_embedder(pooled_projection)  # (B, embedding_dim)

        # æ­¥éª¤5ï¼šèåˆæ—¶é—´ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ï¼ˆåŠ æ³•èåˆï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        conditioning = fused_time_emb + text_emb

        return conditioning

class OminiModel(L.LightningModule):
    def __init__(
        self,
        flux_pipe_id: str,
        lora_path: str = None,
        lora_config: dict = None,
        device: str = "cuda",
        dtype: torch.dtype = torch.bfloat16,
        model_config: dict = {},
        adapter_names: List[str] = [None, None, "default"],
        optimizer_config: dict = None,
        gradient_checkpointing: bool = False,
        time_layers_path : str = None,
        omega_: float = 2,
        kappa: float = 0.9,
    ):
        # Initialize the LightningModule
        super().__init__()
        self.model_config = model_config
        self.optimizer_config = optimizer_config
        # Load the Flux pipeline
        self.flux_pipe: FluxPipeline = FluxPipeline.from_pretrained(
            flux_pipe_id, torch_dtype=dtype
        ).to(device)

        # self.flux_pipe: FluxPipeline = FluxPipeline.from_pretrained(
        #     LOCAL_FLUX_DIR,  # å…³é”®ï¼šæ›¿æ¢ä¸ºæœ¬åœ°ç›®å½•è·¯å¾„
        #     torch_dtype=dtype,
        #     local_files_only=True,
        #     device_map="balanced",
        #     max_memory={0: "12GB", 1: "12GB"},
        # )
        self.transformer = self.flux_pipe.transformer
        # print(f"dtype:{dtype},self.dtype:{self.dtype},{self.flux_pipe.dtype}")
        # flux_pipe  = FluxPipeline.from_pretrained(
        #     LOCAL_FLUX_DIR, torch_dtype=dtype, local_files_only=True,use_auth_token=False
        # ).to(device)
        # print(f"type:{dtype}")
        # target_num_layers = 1  # åŸé»˜è®¤ 19ï¼Œè‡ªå®šä¹‰ä¿®æ”¹
        # target_num_single_layers = 1  # åŸé»˜è®¤ 38ï¼Œè‡ªå®šä¹‰ä¿®æ”¹
        #
        # # ç¬¬ä¸‰æ­¥ï¼šæå–ç°æœ‰ Transformer çš„é…ç½®ï¼ˆå¤ç”¨æ‰€æœ‰å…¶ä»–å‚æ•°ï¼Œä»…ä¿®æ”¹ä¸¤ä¸ªç›®æ ‡å‚æ•°ï¼‰
        # # ç¬¬ä¸‰æ­¥ï¼šæå–ç°æœ‰ Transformer çš„é…ç½®ï¼ˆå¤ç”¨æ‰€æœ‰å…¶ä»–å‚æ•°ï¼Œä»…ä¿®æ”¹ä¸¤ä¸ªç›®æ ‡å‚æ•°ï¼‰
        # original_transformer = flux_pipe.transformer
        # original_state_dict = original_transformer.state_dict()
        # transformer_config = {
        #     "patch_size": 1,  # Flux å›ºå®šä¸º 1
        #     "in_channels": 64,  # Flux å›ºå®šä¸º 64
        #     "out_channels": original_transformer.out_channels,  # éé…ç½®å±æ€§ï¼Œç›´æ¥è®¿é—®ï¼ˆæ— è­¦å‘Šï¼‰
        #     "num_layers": target_num_layers,  # æ›¿æ¢ä¸ºè‡ªå®šä¹‰å€¼
        #     "num_single_layers": target_num_single_layers,  # æ›¿æ¢ä¸ºè‡ªå®šä¹‰å€¼
        #     # ä»¥ä¸‹æ‰€æœ‰é…ç½®å±æ€§ï¼Œå‡æ”¹ä¸ºé€šè¿‡ .config è®¿é—®ï¼ˆæ¶ˆé™¤å¼ƒç”¨è­¦å‘Šï¼‰
        #     "attention_head_dim": original_transformer.config.attention_head_dim,
        #     "num_attention_heads": original_transformer.config.num_attention_heads,
        #     "joint_attention_dim": original_transformer.config.joint_attention_dim,
        #     "pooled_projection_dim": original_transformer.config.pooled_projection_dim,
        #     "guidance_embeds": hasattr(original_transformer.config,
        #                                "guidance_embeds") and original_transformer.config.guidance_embeds,
        #     "axes_dims_rope": original_transformer.pos_embed.axes_dim  # éé…ç½®å±æ€§ï¼Œç›´æ¥è®¿é—®ï¼ˆæ— è­¦å‘Šï¼‰
        # }
        #
        # # ç¬¬å››æ­¥ï¼šé‡æ–°å®ä¾‹åŒ– Transformerï¼ˆä½¿ç”¨ä¿®æ”¹åçš„é…ç½®ï¼‰
        # # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¯¼å…¥ Flux Transformer çš„å®é™…ç±»ï¼ˆé€šå¸¸æ˜¯ FluxTransformerï¼‰
        # from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel
        # new_transformer = FluxTransformer2DModel(**transformer_config).to(device, dtype=dtype)
        #
        # print("è¿‡æ»¤é¢„è®­ç»ƒæƒé‡ï¼Œä»…ä¿ç•™ 1 å±‚å¯¹åº”æƒé‡...")
        # filtered_state_dict = {}
        # for key, value in original_state_dict.items():
        #     # æƒ…å†µ 1ï¼šé transformer_blocks ç›¸å…³çš„é€šç”¨æƒé‡ï¼ˆå…¨éƒ¨ä¿ç•™ï¼Œå¦‚åµŒå…¥å±‚ã€ä½ç½®ç¼–ç ç­‰ï¼‰
        #     if not key.startswith("transformer_blocks."):
        #         filtered_state_dict[key] = value
        #     # æƒ…å†µ 2ï¼štransformer_blocks ç›¸å…³çš„æƒé‡ï¼Œä»…ä¿ç•™ç¬¬ 0 å±‚ï¼ˆå¯¹åº” target_num_layers=1ï¼‰
        #     else:
        #         # æå–å±‚ç´¢å¼•ï¼ˆå¦‚ "transformer_blocks.0.attention.q_proj.weight" ä¸­çš„ 0ï¼‰
        #         layer_index = int(key.split(".")[1])
        #         if layer_index == 0:  # åªä¿ç•™ç¬¬ 0 å±‚ï¼ˆå³ç¬¬ 1 å±‚ï¼Œå¯¹åº” target_num_layers=1ï¼‰
        #             filtered_state_dict[key] = value
        #
        # # æ­¥éª¤ 5ï¼šåŠ è½½è¿‡æ»¤åçš„æƒé‡ï¼ˆstrict=False å¿½ç•¥æ–°æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„å¤šä½™å±‚æƒé‡ï¼‰
        # print("åŠ è½½è¿‡æ»¤åçš„ 1 å±‚æƒé‡...")
        # missing_keys, unexpected_keys = new_transformer.load_state_dict(
        #     filtered_state_dict,
        #     strict=False
        # )
        #
        # # æ‰“å°åŠ è½½æ—¥å¿—ï¼ˆéªŒè¯æ˜¯å¦åªåŠ è½½äº† 1 å±‚æƒé‡ï¼‰
        # print(f"\næƒé‡åŠ è½½å®Œæˆï¼š")
        # print(f"  ç¼ºå¤±é”®ï¼ˆæ–°æ¨¡å‹æœ‰ã€é¢„è®­ç»ƒæ— ï¼Œæ­£å¸¸ï¼‰ï¼š{len(missing_keys)} ä¸ª")
        # print(f"  å¤šä½™é”®ï¼ˆé¢„è®­ç»ƒæœ‰ã€æ–°æ¨¡å‹æ— ï¼Œå·²è¿‡æ»¤ï¼Œæ­£å¸¸ï¼‰ï¼š{len(unexpected_keys)} ä¸ª")
        # if len(missing_keys) > 0 and missing_keys[:5]:
        #     print(f"  ç¼ºå¤±é”®ç¤ºä¾‹ï¼š{missing_keys[:5]}")
        # if len(unexpected_keys) > 0 and unexpected_keys[:5]:
        #     print(f"  å¤šä½™é”®ç¤ºä¾‹ï¼š{unexpected_keys[:5]}")
        #
        # # æ­¥éª¤ 6ï¼šå°† 1 å±‚ Transformer ç§»åˆ° GPUï¼ˆä»…è¿™ä¸€æ­¥å ç”¨ GPU æ˜¾å­˜ï¼‰
        # print("\nå°† 1 å±‚ Transformer ç§»åˆ° GPU...")
        # new_transformer = new_transformer.to(device, dtype=dtype)
        #
        # # æ­¥éª¤ 7ï¼šæ„å»ºè½»é‡ç®¡é“ï¼ˆä»…ä¿ç•™å¿…è¦ç»„ä»¶ï¼Œå…¶ä»–ç§»åˆ° CPUï¼‰
        # print("æ„å»ºè½»é‡ FluxPipeline...")
        # self.flux_pipe = flux_pipe
        # self.flux_pipe.transformer = new_transformer # æ›¿æ¢ä¸º 1 å±‚ Transformer
        # self.transformer = new_transformer
        #
        # # æ­¥éª¤ 8ï¼šéªŒè¯æ˜¾å­˜å ç”¨å’Œæ¨¡å‹å±‚æ•°
        # print(f"\næœ€ç»ˆéªŒè¯ï¼š")
        # print(f"  æ–°æ¨¡å‹ transformer_blocks é•¿åº¦ï¼š{len(self.transformer.transformer_blocks)}")

        # self.transformer = self.flux_pipe.transformer
        self.transformer.gradient_checkpointing = gradient_checkpointing
        self.transformer.train()

        # Freeze the Flux pipeline
        self.flux_pipe.text_encoder.requires_grad_(False).eval()
        self.flux_pipe.text_encoder_2.requires_grad_(False).eval()
        self.flux_pipe.vae.requires_grad_(False).eval()
        self.adapter_names = adapter_names
        self.adapter_set = set([each for each in adapter_names if each is not None])

        # Initialize LoRA layers
        self.time_layers= self.replace_and_freeze_time_text_embed(self.flux_pipe, time_layers_path)

        self.lora_layers = self.init_lora(lora_path, lora_config, device)
        self.omega = omega_*(1-kappa)
        self.kappa = kappa
        # devices = set()
        # for name, param in self.transformer.named_parameters():
        #     devices.add(str(param.device))
        #
        # print(f"æ¨¡å‹åˆ†å¸ƒåœ¨: {devices}")

    # def on_before_optimizer_step(self, optimizer):
    #     # è¿™ä¸ªå‡½æ•°ä¼šåœ¨ optimizer.step() ä¹‹å‰è‡ªåŠ¨è¢«è°ƒç”¨
    #     # æ­¤æ—¶æ¢¯åº¦å·²ç»è®¡ç®—å¥½äº†ï¼Œéå¸¸é€‚åˆæ£€æŸ¥æ¢¯åº¦é—®é¢˜ï¼
    #
    #     print("--- Gradient Check ---")
    #     # æ£€æŸ¥ä½ çš„ MLP å±‚çš„æ¢¯åº¦
    #     for name, param in self.transformer.custom_time_text_embed.time_fusion_mlp.named_parameters():
    #         if param.grad is None:
    #             print(f"âŒ {name}: Grad is None! (Disconnected graph)")
    #         else:
    #             grad_mean = param.grad.abs().mean().item()
    #             grad_max = param.grad.abs().max().item()
    #             print(f"âœ… {name}: Grad Mean={grad_mean:.2e}, Max={grad_max:.2e}")
    #
    #             if grad_mean == 0:
    #                 print(f"âš ï¸ {name}: Grad is ZERO! (Vanishing gradient or precision issue)")
    #
    #         # åªçœ‹å‰å‡ ä¸ªå°±è¡Œï¼Œä¸ç”¨æ‰“å°å…¨éƒ¨
    #         break

    @staticmethod
    def replace_and_freeze_time_text_embed(fluxpipe, time_layers_path):
        """
        æ›¿æ¢ transformer.time_text_embed ä¸º time_text_embed_module2
        ä»…è®­ç»ƒ timestep_embedder å’Œ delta_timestep_embedder
        - timestep_embedder: ä»åŸæ¨¡å‹åŠ è½½æƒé‡
        - delta_timestep_embedder: é›¶åˆå§‹åŒ–
        args:
            fluxpipe: FLUX ç®¡é“å¯¹è±¡ï¼ˆåŒ…å« transformer æ¨¡å—ï¼‰
            time_layers_path: è‡ªå®šä¹‰æ—¶é—´å±‚æƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼Œå½“å‰é€»è¾‘ä¸‹ä»…å…¼å®¹ embedder æƒé‡ï¼‰
        """
        # 1. è·å–åŸæ¨¡å—ï¼ˆç”¨äºç»§æ‰¿ timestep_embedder æƒé‡ï¼‰

        original_embed_module = fluxpipe.transformer.time_text_embed

        # 2. è·å–ç»´åº¦å‚æ•°ï¼ˆä¸åŸæ¨¡å—å¯¹é½ï¼‰
        embedding_dim = fluxpipe.transformer.inner_dim
        pooled_projection_dim = fluxpipe.transformer.config.pooled_projection_dim

        # 3. å®ä¾‹åŒ–ä¿®æ”¹åçš„è‡ªå®šä¹‰æ¨¡å—ï¼ˆtime_text_embed_module2ï¼‰
        custom_embed_module = time_text_embed_module2(
            embedding_dim=embedding_dim,
            pooled_projection_dim=pooled_projection_dim
        )

        # 4. æƒé‡å¤ç”¨ä¸åˆå§‹åŒ–ï¼ˆæ ¸å¿ƒè¦æ±‚ï¼‰
        # 4.1 åŠ è½½ time_proj æƒé‡ï¼ˆt ç¼–ç çš„æŠ•å½±å±‚ï¼Œåç»­å†»ç»“ï¼Œä»…ä¿è¯ç»“æ„å¯¹é½ï¼‰
        custom_embed_module.time_proj.load_state_dict(original_embed_module.time_proj.state_dict())
        # 4.2 timestep_embedder åŠ è½½åŸæ¨¡å‹æƒé‡ï¼ˆç¬¦åˆè¦æ±‚ï¼šç»§æ‰¿åŸæƒé‡ï¼‰
        custom_embed_module.timestep_embedder.load_state_dict(original_embed_module.timestep_embedder.state_dict())
        # 4.3 delta_time_proj å¤ç”¨ time_proj æƒé‡ï¼ˆç»“æ„ä¸€è‡´ï¼Œåç»­å†»ç»“ï¼Œä¸å½±å“è®­ç»ƒï¼‰
        custom_embed_module.delta_time_proj.load_state_dict(original_embed_module.time_proj.state_dict())

        # 4.4 delta_timestep_embedder é›¶åˆå§‹åŒ–ï¼ˆå…³é”®ï¼šä¸åŠ è½½åŸæƒé‡ï¼Œå¼ºåˆ¶ç½®é›¶ï¼‰
        def zero_init_module(module):
            """è¾…åŠ©å‡½æ•°ï¼šå°†æ¨¡å—æ‰€æœ‰å¯å­¦ä¹ å‚æ•°ç½®é›¶"""
            for param in module.parameters():
                nn.init.constant_(param, 0.0)

        zero_init_module(custom_embed_module.delta_timestep_embedder)
        # 4.5 åŠ è½½ text_embedder æƒé‡ï¼ˆæ–‡æœ¬ç¼–ç å±‚ï¼Œåç»­å†»ç»“ï¼‰
        custom_embed_module.text_embedder.load_state_dict(original_embed_module.text_embedder.state_dict())

        # 5. å†»ç»“éç›®æ ‡å±‚ï¼Œä»…è§£å†»ä¸¤ä¸ª embedderï¼ˆæ ¸å¿ƒè®­ç»ƒç›®æ ‡ï¼‰
        # 5.1 å†»ç»“ time_projï¼ˆt æŠ•å½±å±‚ï¼Œæ— è®­ç»ƒéœ€æ±‚ï¼‰
        for param in custom_embed_module.time_proj.parameters():
            param.requires_grad = False
        # 5.2 å†»ç»“ delta_time_projï¼ˆÎ”t æŠ•å½±å±‚ï¼Œæ— è®­ç»ƒéœ€æ±‚ï¼‰
        for param in custom_embed_module.delta_time_proj.parameters():
            param.requires_grad = False
        # 5.3 å†»ç»“ text_embedderï¼ˆæ–‡æœ¬ç¼–ç å±‚ï¼Œæ— è®­ç»ƒéœ€æ±‚ï¼‰
        for param in custom_embed_module.text_embedder.parameters():
            param.requires_grad = False

        # 5.4 æ˜¾å¼è®¾ç½®ä¸¤ä¸ª embedder å¯è®­ç»ƒï¼ˆæ ¸å¿ƒï¼šä»…è¿™ä¸¤ä¸ªå±‚å‚ä¸è®­ç»ƒï¼‰
        # è§£å†» timestep_embedder
        for param in custom_embed_module.timestep_embedder.parameters():
            param.requires_grad = True
        # è§£å†» delta_timestep_embedderï¼ˆé›¶åˆå§‹åŒ–åï¼Œå¼€å¯è®­ç»ƒï¼‰
        for param in custom_embed_module.delta_timestep_embedder.parameters():
            param.requires_grad = True

        # 6. æ›¿æ¢åŸæ¨¡å—ï¼ˆä¿æŒåŸæœ‰æŒ‚è½½æ–¹å¼ï¼Œå…¼å®¹åç»­é€»è¾‘ï¼‰
        fluxpipe.transformer.custom_time_text_embed = custom_embed_module

        # 7. åŠ è½½è‡ªå®šä¹‰æ—¶é—´å±‚æƒé‡ï¼ˆè‹¥è·¯å¾„ä¸ä¸º Noneï¼Œä»…é€‚é…ä¸¤ä¸ª embedder æƒé‡ï¼‰
        # æ³¨æ„ï¼šè‹¥ä½¿ç”¨ä¹‹å‰åˆ†å¼€ä¿å­˜çš„ MLP æƒé‡ï¼Œæ­¤å¤„ä¼šæŠ¥é”™ï¼Œéœ€å¯¹åº”æ›´æ–° OminiModel.load_custom_embed_weights
        if time_layers_path is not None:
            OminiModel.load_custom_embed_weights(fluxpipe.transformer, time_layers_path)

        # 8. æ•´ç†å¹¶è¿”å›æ‰€æœ‰å¯è®­ç»ƒå‚æ•°ï¼ˆä¸¤ä¸ª embedder çš„å‚æ•°åˆ—è¡¨åˆå¹¶ï¼‰
        trainable_params = []
        trainable_params.extend(list(custom_embed_module.timestep_embedder.parameters()))
        trainable_params.extend(list(custom_embed_module.delta_timestep_embedder.parameters()))

        return trainable_params
    # def replace_and_freeze_time_text_embed(fluxpipe, time_layers_path):
    #     """
    #     æ›¿æ¢ transformer.time_text_embed ä¸º time_text_embed_module2ï¼ˆå…¼å®¹æ¨¡å‹å¹¶è¡Œï¼Œè§£å†³ Meta å¼ é‡ no data é—®é¢˜ï¼‰
    #     ä»…è®­ç»ƒ timestep_embedder å’Œ delta_timestep_embedder
    #     - timestep_embedder: ä»åŸæ¨¡å‹åŠ è½½æƒé‡ï¼ˆå…ˆè½åœ° Meta å±‚ï¼Œæå–æœ‰æ•ˆæƒé‡ï¼‰
    #     - delta_timestep_embedder: é›¶åˆå§‹åŒ–
    #     args:
    #         fluxpipe: FLUX ç®¡é“å¯¹è±¡ï¼ˆåŒ…å« transformer æ¨¡å—ï¼Œdevice_map="balanced" ä¸¤å¼ å¡å¹¶è¡Œï¼‰
    #         time_layers_path: è‡ªå®šä¹‰æ—¶é—´å±‚æƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼Œå½“å‰é€»è¾‘ä¸‹ä»…å…¼å®¹ embedder æƒé‡ï¼‰
    #     """
    #
    #     # ------------- æ–°å¢ï¼šæ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼ˆè§£å†³ Meta å¼ é‡é—®é¢˜ï¼Œé€‚é…ä¸¤å¼ å¡å¹¶è¡Œï¼‰-------------
    #     def get_valid_device_and_dtype(module):
    #         """ä»æ¨¡å—ä¸­è‡ªåŠ¨æå–æœ‰æ•ˆè®¾å¤‡ï¼ˆé metaï¼‰å’Œ dtypeï¼Œé¿å…ç¡¬ç¼–ç  GPU"""
    #         for param in module.parameters():
    #             if not param.is_meta and param.device != torch.device('meta'):
    #                 return param.device, param.dtype
    #         # å…œåº•ï¼šä¸¤å¼ å¡é»˜è®¤ä½¿ç”¨ cuda:0ï¼Œä¹Ÿå¯æ”¹ä¸º cuda:1
    #         return torch.device("cuda:0"), torch.float32
    #
    #     def force_land_meta_module(original_module, flux_pipe, target_device, target_dtype):
    #         """ä¸»åŠ¨å°† Meta è®¾å¤‡çš„åŸæ¨¡å—è½åœ°åˆ°å…·ä½“ GPUï¼ŒåŠ è½½çœŸå®æƒé‡æ•°æ®"""
    #         if original_module is None:
    #             raise ValueError("éœ€è¦æå–æƒé‡çš„åŸæ¨¡å—ä¸èƒ½ä¸º None")
    #
    #         # 1. åˆ¤æ–­æ˜¯å¦å·²è½åœ°ï¼ˆé meta è®¾å¤‡ï¼Œæœ‰æœ‰æ•ˆæ•°æ®ï¼‰
    #         has_valid_data = False
    #         for param in original_module.parameters():
    #             if not param.is_meta and param.device != torch.device('meta'):
    #                 has_valid_data = True
    #                 break
    #         if has_valid_data:
    #             print(f"[Meta å±‚è½åœ°] åŸæ¨¡å—å·²åœ¨æœ‰æ•ˆè®¾å¤‡ {target_device}ï¼Œæ— éœ€é‡å¤è½åœ°")
    #             return original_module.to(target_device, dtype=target_dtype, non_blocking=True)
    #
    #         # 2. æ ¸å¿ƒï¼šåˆ©ç”¨ accelerate åŠ è½½æœ¬åœ°æƒé‡å¹¶åˆ†å‘åˆ°ç›®æ ‡ GPUï¼Œè§£å†³ no data é—®é¢˜
    #         try:
    #             landed_module = load_checkpoint_and_dispatch(
    #                 original_module,
    #                 checkpoint=LOCAL_FLUX_DIR,  # å¯¹åº” LOCAL_FLUX_DIR
    #                 device_map={"": target_device},  # ä»…å°†è¯¥æ¨¡å—åˆ†å‘åˆ°ç›®æ ‡ GPUï¼Œä¸ç ´åæ•´ä½“å¹¶è¡Œ
    #                 dtype=target_dtype,
    #                 local_files_only=True,
    #                 skip_keys=None
    #             )
    #             print(f"[Meta å±‚è½åœ°] åŸæ¨¡å—å·²æˆåŠŸè½åœ°åˆ° {target_device}ï¼ŒåŠ è½½çœŸå®æƒé‡")
    #             return landed_module
    #         except Exception as e:
    #             print(f"[Meta å±‚è½åœ°] è‡ªåŠ¨åŠ è½½å¤±è´¥ï¼Œå…œåº•è¿ç§»è®¾å¤‡ï¼š{e}")
    #             # å…œåº•ï¼šæ‰‹åŠ¨è¿ç§»è®¾å¤‡ï¼ˆä»…åˆ†é…å†…å­˜ï¼Œæ— çœŸå®æ•°æ®ï¼Œé¿å…æµç¨‹ä¸­æ–­ï¼‰
    #             original_module.to_empty(device=target_device)
    #             return original_module
    #
    #     def safe_extract_state_dict(module):
    #         """ä»è½åœ°åçš„æ¨¡å—ä¸­å®‰å…¨æå–æƒé‡ï¼Œè¿‡æ»¤æ®‹ç•™ Meta å¼ é‡"""
    #         valid_state_dict = {}
    #         raw_state_dict = module.state_dict()
    #         for key, param in raw_state_dict.items():
    #             if not param.is_meta and param.device != torch.device('meta'):
    #                 # å…‹éš†å‚æ•°ï¼Œé¿å…ä¿®æ”¹åŸæ¨¡å—æƒé‡
    #                 valid_state_dict[key] = param.detach().clone()
    #             else:
    #                 print(f"[å®‰å…¨ææƒ] è·³è¿‡æ®‹ç•™ Meta å¼ é‡å‚æ•°ï¼š{key}")
    #         return valid_state_dict
    #
    #     # ------------- æ­¥éª¤ 1ï¼šæå–æœ‰æ•ˆè®¾å¤‡å’Œ dtypeï¼Œé€‚é…ä¸¤å¼ å¡å¹¶è¡Œ -------------
    #     valid_device, valid_dtype = get_valid_device_and_dtype(fluxpipe.transformer)
    #     print(f"[æµç¨‹åˆå§‹åŒ–] æå–åˆ°æœ‰æ•ˆè®¾å¤‡ï¼š{valid_device}ï¼Œdtypeï¼š{valid_dtype}")
    #
    #     # ------------- æ­¥éª¤ 2ï¼šè·å–åŸæ¨¡å—å¹¶ä¸»åŠ¨è½åœ° Meta å±‚ï¼ˆæ ¸å¿ƒï¼šè§£å†³ no dataï¼‰-------------
    #     original_embed_module = fluxpipe.transformer.time_text_embed
    #     # ä¸»åŠ¨è½åœ°åŸæ¨¡å—ï¼ŒåŠ è½½çœŸå®æƒé‡æ•°æ®
    #     landed_original_embed_module = force_land_meta_module(
    #         original_embed_module,
    #         fluxpipe,
    #         valid_device,
    #         valid_dtype
    #     )
    #
    #     # ------------- æ­¥éª¤ 3ï¼šè·å–ç»´åº¦å‚æ•°ï¼ˆä¸åŸæ¨¡å—å¯¹é½ï¼Œä¿ç•™åŸæœ‰é€»è¾‘ï¼‰-------------
    #     embedding_dim = fluxpipe.transformer.inner_dim
    #     pooled_projection_dim = fluxpipe.transformer.config.pooled_projection_dim
    #
    #     # ------------- æ­¥éª¤ 4ï¼šå®ä¾‹åŒ–è‡ªå®šä¹‰æ¨¡å—ï¼ˆè½åœ°åˆ°æœ‰æ•ˆ GPUï¼Œé¿å… Meta å ä½ï¼‰-------------
    #     custom_embed_module = time_text_embed_module2(
    #         embedding_dim=embedding_dim,
    #         pooled_projection_dim=pooled_projection_dim
    #     ).to(  # ç›´æ¥è¿ç§»åˆ°æœ‰æ•ˆè®¾å¤‡ï¼Œå…¼å®¹ä¸¤å¼ å¡å¹¶è¡Œ
    #         device=valid_device,
    #         dtype=valid_dtype,
    #         non_blocking=True
    #     )
    #
    #     # ------------- æ­¥éª¤ 5ï¼šæƒé‡å¤ç”¨ä¸åˆå§‹åŒ–ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šå®‰å…¨æå–æƒé‡ï¼Œè§£å†³ Meta é—®é¢˜ï¼‰-------------
    #     # 5.1 å®‰å…¨æå–åŸæ¨¡å—å„ç»„ä»¶çš„æœ‰æ•ˆæƒé‡ï¼ˆé¿å… no data æŠ¥é”™ï¼‰
    #     original_time_proj_sd = safe_extract_state_dict(landed_original_embed_module.time_proj)
    #     original_timestep_embed_sd = safe_extract_state_dict(landed_original_embed_module.timestep_embedder)
    #     original_text_embed_sd = safe_extract_state_dict(landed_original_embed_module.text_embedder)
    #
    #     # 5.2 åŠ è½½ time_proj æƒé‡ï¼ˆt ç¼–ç çš„æŠ•å½±å±‚ï¼Œåç»­å†»ç»“ï¼Œä»…ä¿è¯ç»“æ„å¯¹é½ï¼‰
    #     if original_time_proj_sd:
    #         custom_embed_module.time_proj.load_state_dict(original_time_proj_sd, strict=False)
    #     else:
    #         print(f"[æƒé‡åŠ è½½] time_proj æ— æœ‰æ•ˆæƒé‡ï¼Œä½¿ç”¨é»˜è®¤åˆå§‹åŒ–")
    #
    #     # 5.3 timestep_embedder åŠ è½½åŸæ¨¡å‹æƒé‡ï¼ˆç¬¦åˆè¦æ±‚ï¼šç»§æ‰¿åŸæƒé‡ï¼Œå…¼å®¹ Meta å¼ é‡ï¼‰
    #     if original_timestep_embed_sd:
    #         custom_embed_module.timestep_embedder.load_state_dict(original_timestep_embed_sd, strict=False)
    #     else:
    #         print(f"[æƒé‡åŠ è½½] timestep_embedder æ— æœ‰æ•ˆæƒé‡ï¼Œä½¿ç”¨é»˜è®¤åˆå§‹åŒ–")
    #
    #     # 5.4 delta_time_proj å¤ç”¨ time_proj æƒé‡ï¼ˆç»“æ„ä¸€è‡´ï¼Œåç»­å†»ç»“ï¼Œä¸å½±å“è®­ç»ƒï¼‰
    #     if original_time_proj_sd:
    #         custom_embed_module.delta_time_proj.load_state_dict(original_time_proj_sd, strict=False)
    #     else:
    #         print(f"[æƒé‡åŠ è½½] delta_time_proj æ— æœ‰æ•ˆæƒé‡ï¼Œä½¿ç”¨é»˜è®¤åˆå§‹åŒ–")
    #
    #     # 5.5 delta_timestep_embedder é›¶åˆå§‹åŒ–ï¼ˆå…³é”®ï¼šä¸åŠ è½½åŸæƒé‡ï¼Œå¼ºåˆ¶ç½®é›¶ï¼Œä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
    #     def zero_init_module(module):
    #         """è¾…åŠ©å‡½æ•°ï¼šå°†æ¨¡å—æ‰€æœ‰å¯å­¦ä¹ å‚æ•°ç½®é›¶ï¼ˆå…¼å®¹ Meta å¼ é‡ï¼Œä»…æ“ä½œæœ‰æ•ˆå‚æ•°ï¼‰"""
    #         for param in module.parameters():
    #             if not param.is_meta:  # è·³è¿‡ Meta å¼ é‡ï¼Œé¿å…æŠ¥é”™
    #                 nn.init.constant_(param, 0.0)
    #
    #     zero_init_module(custom_embed_module.delta_timestep_embedder)
    #
    #     # 5.6 åŠ è½½ text_embedder æƒé‡ï¼ˆæ–‡æœ¬ç¼–ç å±‚ï¼Œåç»­å†»ç»“ï¼Œå…¼å®¹ Meta å¼ é‡ï¼‰
    #     if original_text_embed_sd:
    #         custom_embed_module.text_embedder.load_state_dict(original_text_embed_sd, strict=False)
    #     else:
    #         print(f"[æƒé‡åŠ è½½] text_embedder æ— æœ‰æ•ˆæƒé‡ï¼Œä½¿ç”¨é»˜è®¤åˆå§‹åŒ–")
    #
    #     # ------------- æ­¥éª¤ 6ï¼šå†»ç»“éç›®æ ‡å±‚ï¼Œä»…è§£å†»ä¸¤ä¸ª embedderï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼Œå¢åŠ  Meta å…¼å®¹ï¼‰-------------
    #     def safe_freeze_module(module):
    #         """å®‰å…¨å†»ç»“æ¨¡å—ï¼Œè·³è¿‡ Meta å¼ é‡ï¼Œé¿å…æŠ¥é”™"""
    #         for param in module.parameters():
    #             if not param.is_meta:
    #                 param.requires_grad = False
    #
    #     def safe_unfreeze_module(module):
    #         """å®‰å…¨è§£å†»æ¨¡å—ï¼Œè·³è¿‡ Meta å¼ é‡ï¼Œé¿å…æŠ¥é”™"""
    #         for param in module.parameters():
    #             if not param.is_meta:
    #                 param.requires_grad = True
    #
    #     # 6.1 å†»ç»“ time_projï¼ˆt æŠ•å½±å±‚ï¼Œæ— è®­ç»ƒéœ€æ±‚ï¼‰
    #     safe_freeze_module(custom_embed_module.time_proj)
    #     # 6.2 å†»ç»“ delta_time_projï¼ˆÎ”t æŠ•å½±å±‚ï¼Œæ— è®­ç»ƒéœ€æ±‚ï¼‰
    #     safe_freeze_module(custom_embed_module.delta_time_proj)
    #     # 6.3 å†»ç»“ text_embedderï¼ˆæ–‡æœ¬ç¼–ç å±‚ï¼Œæ— è®­ç»ƒéœ€æ±‚ï¼‰
    #     safe_freeze_module(custom_embed_module.text_embedder)
    #
    #     # 6.4 æ˜¾å¼è®¾ç½®ä¸¤ä¸ª embedder å¯è®­ç»ƒï¼ˆæ ¸å¿ƒï¼šä»…è¿™ä¸¤ä¸ªå±‚å‚ä¸è®­ç»ƒï¼‰
    #     # è§£å†» timestep_embedder
    #     safe_unfreeze_module(custom_embed_module.timestep_embedder)
    #     # è§£å†» delta_timestep_embedderï¼ˆé›¶åˆå§‹åŒ–åï¼Œå¼€å¯è®­ç»ƒï¼‰
    #     safe_unfreeze_module(custom_embed_module.delta_timestep_embedder)
    #
    #     # ------------- æ­¥éª¤ 7ï¼šæ›¿æ¢åŸæ¨¡å—ï¼ˆä¿æŒåŸæœ‰æŒ‚è½½æ–¹å¼ï¼Œå…¼å®¹æ¨¡å‹å¹¶è¡Œï¼‰-------------
    #     # å…³é”®ï¼šç»‘å®šå‰å†æ¬¡ç¡®è®¤è®¾å¤‡ï¼Œä¿è¯è‡ªå®šä¹‰æ¨¡å—é€‚é…ä¸¤å¼ å¡çš„å¹¶è¡Œåˆ†å¸ƒ
    #     fluxpipe.transformer.custom_time_text_embed = custom_embed_module.to(
    #         device=valid_device,
    #         dtype=valid_dtype,
    #         non_blocking=True
    #     )
    #     print(f"[æ¨¡å—æ›¿æ¢] è‡ªå®šä¹‰æ¨¡å— custom_time_text_embed å·²æˆåŠŸç»‘å®šåˆ° transformer")
    #
    #     # ------------- æ­¥éª¤ 8ï¼šåŠ è½½è‡ªå®šä¹‰æ—¶é—´å±‚æƒé‡ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼Œå¢åŠ  Meta å…¼å®¹ï¼‰-------------
    #     # æ³¨æ„ï¼šè‹¥ä½¿ç”¨ä¹‹å‰åˆ†å¼€ä¿å­˜çš„ MLP æƒé‡ï¼Œæ­¤å¤„ä¼šæŠ¥é”™ï¼Œéœ€å¯¹åº”æ›´æ–° OminiModel.load_custom_embed_weights
    #     if time_layers_path is not None and isinstance(time_layers_path, str):
    #         # ä¼ é€’æœ‰æ•ˆè®¾å¤‡ï¼Œé¿å…åŠ è½½æƒé‡æ—¶å‡ºç° Meta å¼ é‡/è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
    #         OminiModel.load_custom_embed_weights(
    #             fluxpipe.transformer,
    #             time_layers_path,
    #             map_location=valid_device  # æ–°å¢ï¼šé€‚é…æ¨¡å‹å¹¶è¡Œï¼Œéœ€ä¿è¯ load_custom_embed_weights æ”¯æŒè¯¥å‚æ•°
    #         )
    #
    #     # ------------- æ­¥éª¤ 9ï¼šæ•´ç†å¹¶è¿”å›æ‰€æœ‰å¯è®­ç»ƒå‚æ•°ï¼ˆè¿‡æ»¤ Meta å¼ é‡ï¼Œå…¼å®¹å¹¶è¡Œï¼‰-------------
    #     trainable_params = []
    #
    #     def collect_valid_trainable_params(module, param_list):
    #         """æ”¶é›†æœ‰æ•ˆå¯è®­ç»ƒå‚æ•°ï¼Œè·³è¿‡ Meta å¼ é‡ï¼Œé¿å…æ— æ•ˆå‚æ•°ä¼ å…¥ä¼˜åŒ–å™¨"""
    #         for param in module.parameters():
    #             if param.requires_grad and not param.is_meta:
    #                 param_list.append(param)
    #
    #     collect_valid_trainable_params(custom_embed_module.timestep_embedder, trainable_params)
    #     collect_valid_trainable_params(custom_embed_module.delta_timestep_embedder, trainable_params)
    #
    #     print(f"[æµç¨‹å®Œæˆ] å¯è®­ç»ƒå‚æ•°æ€»æ•°ï¼š{len(trainable_params)}ï¼Œå·²æˆåŠŸè¿”å›")
    #     return trainable_params



    def verify_module_params(self):
        custom_module=self.transformer.custom_time_text_embed
        """éªŒè¯è‡ªå®šä¹‰æ¨¡å—çš„å‚æ•°å¯è®­ç»ƒçŠ¶æ€"""
        print("=== è‡ªå®šä¹‰æ¨¡å—å‚æ•°å¯è®­ç»ƒçŠ¶æ€ ===")
        for name, param in custom_module.named_parameters():
            print(f"å‚æ•°å: {name:<60} å¯è®­ç»ƒ: {param.requires_grad}")

        # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in custom_module.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in custom_module.parameters())
        print(f"\nå¯è®­ç»ƒå‚æ•°æ€»æ•°: {trainable_params:,} / æ€»å‚æ•°æ•°: {total_params:,}")

    def _print_trainable_params_details(self):
        """
        è¾…åŠ©å‡½æ•°ï¼šæ‰“å°æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
        - å‚æ•°åç§°
        - å‚æ•°å½¢çŠ¶
        - å‚æ•°ç±»å‹ï¼ˆLoRA/è‡ªå®šä¹‰ time_fusion_mlp/å…¶ä»–ï¼‰
        - å¯è®­ç»ƒå‚æ•°æ€»æ•°/æ€»å‚æ•°é‡
        """
        # æ”¶é›†å¯è®­ç»ƒå‚æ•°çš„åç§°ã€å½¢çŠ¶ã€ç±»å‹
        trainable_params_info = []
        total_trainable_params = 0  # å¯è®­ç»ƒå‚æ•°æ€»æ•°ï¼ˆå…ƒç´ ä¸ªæ•°ï¼‰
        total_model_params = 0  # æ¨¡å‹æ€»å‚æ•°æ•°ï¼ˆå…ƒç´ ä¸ªæ•°ï¼‰

        for name, param in self.transformer.named_parameters():
            # ç»Ÿè®¡æ¨¡å‹æ€»å‚æ•°æ•°
            total_model_params += param.numel()

            # ä»…å¤„ç†å¯è®­ç»ƒå‚æ•°
            if param.requires_grad:
                # åˆ¤æ–­å‚æ•°ç±»å‹
                if "time_fusion_mlp" in name:
                    param_type = "è‡ªå®šä¹‰ time_fusion_mlp"
                    # è®°å½•å‚æ•°ä¿¡æ¯
                    trainable_params_info.append({
                        "name": name,
                        "shape": list(param.shape),
                        "type": param_type,
                        "numel": param.numel()  # è¯¥å‚æ•°çš„å…ƒç´ ä¸ªæ•°
                    })
                elif "custom_time_text_embed" in name:
                    param_type = "è‡ªå®šä¹‰ time_text_embedï¼ˆéèåˆå±‚ï¼‰"
                    # è®°å½•å‚æ•°ä¿¡æ¯
                    trainable_params_info.append({
                        "name": name,
                        "shape": list(param.shape),
                        "type": param_type,
                        "numel": param.numel()  # è¯¥å‚æ•°çš„å…ƒç´ ä¸ªæ•°
                    })



                # ç´¯åŠ å¯è®­ç»ƒå‚æ•°æ€»æ•°
                total_trainable_params += param.numel()

        # æ‰“å°æ ‡é¢˜
        print("\n" + "=" * 80)
        print("ğŸ“Œ å¯è®­ç»ƒå‚æ•°è¯¦æƒ…")
        print("=" * 80)

        # æ‰“å°æ¯ä¸ªå¯è®­ç»ƒå‚æ•°
        if trainable_params_info:
            for idx, info in enumerate(trainable_params_info, 1):
                print(f"\n[{idx}] å‚æ•°åç§°: {info['name']}")
                print(f"   å½¢çŠ¶: {info['shape']}")
                print(f"   ç±»å‹: {info['type']}")
                print(f"   å…ƒç´ ä¸ªæ•°: {info['numel']:,}")
        else:
            print("\nâš ï¸  æœªæ£€æµ‹åˆ°ä»»ä½•å¯è®­ç»ƒå‚æ•°ï¼")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "-" * 80)
        print(f"ğŸ“Š ç»Ÿè®¡æ±‡æ€»:")
        print(f"   å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ˆä¸ªï¼‰: {len(trainable_params_info)}")
        print(f"   å¯è®­ç»ƒå‚æ•°æ€»å…ƒç´ æ•°: {total_trainable_params:,}")
        print(f"   æ¨¡å‹æ€»å‚æ•°å…ƒç´ æ•°: {total_model_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°å æ¯”: {total_trainable_params / total_model_params * 100:.4f}%")
        print("=" * 80 + "\n")



    def init_lora(self, lora_path: str, lora_config: dict, device):
        assert lora_path or lora_config
        if lora_path:
            for adapter_name in self.adapter_set:
                lora_file = os.path.join(lora_path, f"{adapter_name}.safetensors")

                if not os.path.exists(lora_file):
                    raise FileNotFoundError(f"LoRA file not found: {lora_file}")

                # ä½¿ç”¨ä¼ å…¥çš„ lora_config æ·»åŠ é€‚é…å™¨
                self.transformer.add_adapter(
                    LoraConfig(**lora_config), adapter_name=adapter_name
                )

                # åŠ è½½æƒé‡
                lora_state_dict = load_file(lora_file)
                set_peft_model_state_dict(
                    self.transformer,
                    lora_state_dict,
                    adapter_name=adapter_name
                )

            lora_layers = filter(
                lambda p: p.requires_grad, self.transformer.parameters()
            )
            print(f"load lora from {lora_path}")
        else:
            for adapter_name in self.adapter_set:
                self.transformer.add_adapter(
                    LoraConfig(**lora_config), adapter_name=adapter_name
                )

            # TODO: Check if this is correct (p.requires_grad)
            lora_layers = filter(
                lambda p: p.requires_grad, self.transformer.parameters()
            )
            # if device=="cuda:0":
            #     self._print_trainable_params_details()
        return list(lora_layers)

    def save_lora(self, path: str):
        for adapter_name in self.adapter_set:
            FluxPipeline.save_lora_weights(
                save_directory=path,
                weight_name=f"{adapter_name}.safetensors",
                transformer_lora_layers=get_peft_model_state_dict(
                    self.transformer, adapter_name=adapter_name
                ),
                safe_serialization=True,
            )

    def save_custom_embed_weights(self, save_directory: str,
                                  timestep_embedder_weight_name: str = "timestep_embedder_weights.safetensors",
                                  delta_timestep_embedder_weight_name: str = "delta_timestep_embedder_weights.safetensors"):
        """
        åˆ†å¼€ä¿å­˜è‡ªå®šä¹‰æ¨¡å—ä¸­ä¸¤ä¸ªå¯è®­ç»ƒ embedder çš„æƒé‡ï¼ˆå„è‡ªä¸ºç‹¬ç«‹æ–‡ä»¶ï¼‰
        é€‚é… time_text_embed_module2ï¼Œä¿å­˜ timestep_embedder å’Œ delta_timestep_embedder
        args:
            save_directory: ä¿å­˜ç›®å½•
            timestep_embedder_weight_name: timestep_embedder çš„æƒé‡æ–‡ä»¶å
            delta_timestep_embedder_weight_name: delta_timestep_embedder çš„æƒé‡æ–‡ä»¶å
        """
        # 1. æ£€æŸ¥è‡ªå®šä¹‰æ¨¡å—æ˜¯å¦å­˜åœ¨
        if not hasattr(self.transformer, "custom_time_text_embed"):
            print("Warning: custom_time_text_embed not found, skipping save.")
            return

        # 2. è·å–è‡ªå®šä¹‰æ¨¡å—
        custom_module = self.transformer.custom_time_text_embed

        # 3. ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(save_directory, exist_ok=True)

        # 4. ä¿å­˜ timestep_embedder æƒé‡ï¼ˆç‹¬ç«‹æ–‡ä»¶ï¼Œæ›¿æ¢åŸ MLP é€»è¾‘ï¼‰
        timestep_embedder_state = custom_module.timestep_embedder.state_dict()
        timestep_save_path = os.path.join(save_directory, timestep_embedder_weight_name)
        save_file(timestep_embedder_state, timestep_save_path)
        print(f"Timestep Embedder weights saved to {timestep_save_path}")

        # 5. ä¿å­˜ delta_timestep_embedder æƒé‡ï¼ˆç‹¬ç«‹æ–‡ä»¶ï¼Œæ›¿æ¢åŸ MLP é€»è¾‘ï¼‰
        delta_timestep_embedder_state = custom_module.delta_timestep_embedder.state_dict()
        delta_timestep_save_path = os.path.join(save_directory, delta_timestep_embedder_weight_name)
        save_file(delta_timestep_embedder_state, delta_timestep_save_path)
        print(f"Delta Timestep Embedder weights saved to {delta_timestep_save_path}")

    @staticmethod
    def load_custom_embed_weights(transformer, load_directory: str,
                                  timestep_embedder_weight_name: str = "timestep_embedder_weights.safetensors",
                                  delta_timestep_embedder_weight_name: str = "delta_timestep_embedder_weights.safetensors"):
        """
        åˆ†å¼€åŠ è½½ä¸¤ä¸ªç‹¬ç«‹ embedder çš„æƒé‡ï¼ˆå¯¹åº”ä¸¤ä¸ªç‹¬ç«‹æ–‡ä»¶ï¼Œé€‚é… time_text_embed_module2ï¼‰
        æ³¨æ„ï¼šå¿…é¡»å…ˆè°ƒç”¨ replace_and_freeze_time_text_embed åˆå§‹åŒ–ç»“æ„åï¼Œæ‰èƒ½è°ƒç”¨æ­¤æ–¹æ³•åŠ è½½æƒé‡
        """
        # 1. æ£€æŸ¥ç»“æ„æ˜¯å¦å·²ç»åˆå§‹åŒ–
        if not hasattr(transformer, "custom_time_text_embed"):
            raise RuntimeError(
                "custom_time_text_embed not initialized. "
                "Please run `replace_and_freeze_time_text_embed` before loading weights."
            )

        # 2. è·å–è‡ªå®šä¹‰æ¨¡å—
        custom_module = transformer.custom_time_text_embed

        # 3. åŠ è½½ timestep_embedder æƒé‡ï¼ˆç‹¬ç«‹æ–‡ä»¶ï¼Œæ›¿æ¢åŸ MLP é€»è¾‘ï¼‰
        # 3.1 æ£€æŸ¥ timestep Embedder æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        timestep_load_path = os.path.join(load_directory, timestep_embedder_weight_name)
        if not os.path.exists(timestep_load_path):
            raise FileNotFoundError(f"Timestep Embedder weight file not found: {timestep_load_path}")
        # 3.2 åŠ è½½å¹¶å†™å…¥ timestep_embedder
        timestep_embedder_state = load_file(timestep_load_path)
        msg1 = custom_module.timestep_embedder.load_state_dict(timestep_embedder_state, strict=True)

        # 4. åŠ è½½ delta_timestep_embedder æƒé‡ï¼ˆç‹¬ç«‹æ–‡ä»¶ï¼Œæ›¿æ¢åŸ MLP é€»è¾‘ï¼‰
        # 4.1 æ£€æŸ¥ delta_timestep Embedder æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        delta_timestep_load_path = os.path.join(load_directory, delta_timestep_embedder_weight_name)
        if not os.path.exists(delta_timestep_load_path):
            raise FileNotFoundError(f"Delta Timestep Embedder weight file not found: {delta_timestep_load_path}")
        # 4.2 åŠ è½½å¹¶å†™å…¥ delta_timestep_embedder
        delta_timestep_embedder_state = load_file(delta_timestep_load_path)
        msg2 = custom_module.delta_timestep_embedder.load_state_dict(delta_timestep_embedder_state, strict=True)

        # 5. æ‰“å°åŠ è½½ç»“æœ
        print(f"Loaded all custom time Embedder weights from {load_directory}")
        print(f"Timestep Embedder load result: {msg1}")
        print(f"Delta Timestep Embedder load result: {msg2}")

    def configure_optimizers(self):
        # Freeze the transformer
        self.transformer.requires_grad_(False)
        opt_config = self.optimizer_config

        # Set the trainable parameters
        self.trainable_params = self.lora_layers + self.time_layers
        # self.trainable_params = self.lora_layers
        # Unfreeze trainable parameters
        for p in self.trainable_params:
            p.requires_grad_(True)

        # mlp_lr = opt_config["params"]["lr"] * 2
        # param_groups = [
        #     {
        #         "params": self.time_layers,
        #         "lr": mlp_lr,
        #         "betas": opt_config["params"]["betas"],
        #         "weight_decay": opt_config["params"]["weight_decay"]
        #     },
        #     {
        #         "params": self.lora_layers,
        #         **opt_config["params"],# æ²¿ç”¨åŸæœ‰é…ç½®
        #     }
        # ]
        # # Initialize the optimizer
        if opt_config["type"] == "AdamW":
            optimizer = torch.optim.AdamW(self.trainable_params, **opt_config["params"])
            # optimizer = torch.optim.AdamW(param_groups)
        elif opt_config["type"] == "Prodigy":
            optimizer = prodigyopt.Prodigy(
                self.trainable_params,
                **opt_config["params"],
            )
        elif opt_config["type"] == "SGD":
            optimizer = torch.optim.SGD(self.trainable_params, **opt_config["params"])
        else:
            raise NotImplementedError("Optimizer not implemented.")
        return optimizer

    # def training_step(self, batch, batch_idx):
    #     imgs, prompts = batch["image"], batch["description"]
    #     image_latent_mask = batch.get("image_latent_mask", None)
    #
    #     # Get the conditions and position deltas from the batch
    #     conditions, position_deltas, position_scales, latent_masks = [], [], [], []
    #     for i in range(1000):
    #         if f"condition_{i}" not in batch:
    #             break
    #         conditions.append(batch[f"condition_{i}"])
    #         position_deltas.append(batch.get(f"position_delta_{i}", [[0, 0]]))
    #         position_scales.append(batch.get(f"position_scale_{i}", [1.0])[0])
    #         latent_masks.append(batch.get(f"condition_latent_mask_{i}", None))
    #
    #     # Prepare inputs
    #     with torch.no_grad():
    #         # Prepare image input
    #         x_0, img_ids = encode_images(self.flux_pipe, imgs)
    #
    #         # Prepare text input
    #         (
    #             prompt_embeds,
    #             pooled_prompt_embeds,
    #             text_ids,
    #         ) = self.flux_pipe.encode_prompt(
    #             prompt=prompts,
    #             prompt_2=None,
    #             prompt_embeds=None,
    #             pooled_prompt_embeds=None,
    #             device=self.flux_pipe.device,
    #             num_images_per_prompt=1,
    #             max_sequence_length=self.model_config.get("max_sequence_length", 512),
    #             lora_scale=None,
    #         )
    #
    #         # Prepare t and x_t
    #         t = torch.sigmoid(torch.randn((imgs.shape[0],), device=self.device))
    #         x_1 = torch.randn_like(x_0).to(self.device)
    #         t_ = t.unsqueeze(1).unsqueeze(1)
    #         x_t = ((1 - t_) * x_0 + t_ * x_1).to(self.dtype)
    #         if image_latent_mask is not None:
    #             x_0 = x_0[:, image_latent_mask[0]]
    #             x_1 = x_1[:, image_latent_mask[0]]
    #             x_t = x_t[:, image_latent_mask[0]]
    #             img_ids = img_ids[image_latent_mask[0]]
    #
    #         # Prepare conditions
    #         condition_latents, condition_ids = [], []
    #         for cond, p_delta, p_scale, latent_mask in zip(
    #             conditions, position_deltas, position_scales, latent_masks
    #         ):
    #             # Prepare conditions
    #             c_latents, c_ids = encode_images(self.flux_pipe, cond)
    #             # Scale the position (see OminiConrtol2)
    #             if p_scale != 1.0:
    #                 scale_bias = (p_scale - 1.0) / 2
    #                 c_ids[:, 1:] *= p_scale
    #                 c_ids[:, 1:] += scale_bias
    #             # Add position delta (see OminiControl)
    #             c_ids[:, 1] += p_delta[0][0]
    #             c_ids[:, 2] += p_delta[0][1]
    #             if len(p_delta) > 1:
    #                 print("Warning: only the first position delta is used.")
    #             # Append to the list
    #             if latent_mask is not None:
    #                 c_latents, c_ids = c_latents[latent_mask], c_ids[latent_mask[0]]
    #             condition_latents.append(c_latents)
    #             condition_ids.append(c_ids)
    #
    #         # Prepare guidance
    #         guidance = (
    #             torch.ones_like(t).to(self.device)
    #             if self.transformer.config.guidance_embeds #é»˜è®¤false
    #             else None
    #         )
    #
    #     branch_n = 2 + len(conditions)
    #     group_mask = torch.ones([branch_n, branch_n], dtype=torch.bool).to(self.device)
    #     # Disable the attention cross different condition branches
    #     group_mask[2:, 2:] = torch.diag(torch.tensor([1] * len(conditions)))
    #     # Disable the attention from condition branches to image branch and text branch
    #     if self.model_config.get("independent_condition", False):
    #         group_mask[2:, :2] = False
    #
    #     # Forward pass
    #     transformer_out = transformer_forward(
    #         self.transformer,
    #         image_features=[x_t, *(condition_latents)],
    #         text_features=[prompt_embeds],
    #         img_ids=[img_ids, *(condition_ids)],
    #         txt_ids=[text_ids],
    #         # There are three timesteps for the three branches
    #         # (text, image, and the condition)
    #         timesteps=[t, t] + [torch.zeros_like(t)] * len(conditions),
    #         # Same as above
    #         pooled_projections=[pooled_prompt_embeds] * branch_n,
    #         guidances=[guidance] * branch_n,
    #         # The LoRA adapter names of each branch
    #         adapters=self.adapter_names,
    #         return_dict=False,
    #         group_mask=group_mask,
    #     )
    #     pred = transformer_out[0]
    #
    #     # Compute loss
    #     step_loss = torch.nn.functional.mse_loss(pred, (x_1 - x_0), reduction="mean")
    #     self.last_t = t.mean().item()
    #
    #     self.log_loss = (
    #         step_loss.item()
    #         if not hasattr(self, "log_loss")
    #         else self.log_loss * 0.95 + step_loss.item() * 0.05
    #     )
    #     return step_loss
    def training_step(self, batch, batch_idx):
        imgs, prompts = batch["image"], batch["description"]
        image_latent_mask = batch.get("image_latent_mask", None)

        # Get the conditions and position deltas from the batch
        conditions, position_deltas, position_scales, latent_masks = [], [], [], []
        for i in range(1000):
            if f"condition_{i}" not in batch:
                break
            conditions.append(batch[f"condition_{i}"])
            position_deltas.append(batch.get(f"position_delta_{i}", [[0, 0]]))
            position_scales.append(batch.get(f"position_scale_{i}", [1.0])[0])
            latent_masks.append(batch.get(f"condition_latent_mask_{i}", None))
        local_rank = get_rank()
        # Prepare inputs
        with torch.no_grad():
            # Prepare image input
            x_0, img_ids = encode_images(self.flux_pipe, imgs)
            x_0 = x_0.to(self.device)
            img_ids = img_ids.to(self.device)
            # Prepare text input
            (
                prompt_embeds,
                pooled_prompt_embeds,
                text_ids,
            ) = self.flux_pipe.encode_prompt(
                prompt=prompts,
                prompt_2=None,
                prompt_embeds=None,
                pooled_prompt_embeds=None,
                device=self.flux_pipe.device,
                num_images_per_prompt=1,
                max_sequence_length=self.model_config.get("max_sequence_length", 512),
                lora_scale=None,
            )

            # -------------------------- Mean Flows æ”¹åŠ¨1: t, r é‡‡æ ·ï¼ˆlognorm(-0.4, 1.0) + 25% râ‰ tï¼‰--------------------------
            # 1. å®šä¹‰ logit-normal é‡‡æ ·å™¨ï¼ˆå…ˆé‡‡æ ·æ­£æ€åˆ†å¸ƒï¼Œå†é€šè¿‡logisticå‡½æ•°æ˜ å°„åˆ°(0,1)ï¼‰
            def lognorm_sampler(batch_size, mu=-0.4, sigma=1.0, device=None):
                normal_dist = Normal(mu, sigma)
                logits = normal_dist.sample((batch_size,))
                return torch.sigmoid(logits).to(device)

            batch_size = imgs.shape[0]
            # é‡‡æ · t å’Œ rï¼ˆç‹¬ç«‹é‡‡æ ·åä¿è¯ t > rï¼‰
            t_raw = lognorm_sampler(batch_size, device=self.device)
            r_raw = lognorm_sampler(batch_size, device=self.device)
            t = torch.max(t_raw, r_raw)
            r = torch.min(t_raw, r_raw)
            # print(f"before:t:{t},{t.dtype},{t.shape}")
            # print(f"before:r:{r},{r.dtype},{r.shape}")
            # 25% æ¦‚ç‡è®© r â‰  tï¼ˆè®ºæ–‡Tab.1aæœ€ä¼˜é…ç½®ï¼‰
            r_eq_t_mask = torch.rand(batch_size, device=self.device) > 0.25
            r[r_eq_t_mask] = t[r_eq_t_mask]
            r = r.to(self.flux_pipe.dtype)
            t = t.to(self.flux_pipe.dtype)
            # print(f"after:t:{t},{t.dtype},{t.shape},{self.flux_pipe.dtype}")
            # print(f"after:r:{r},{r.dtype},{r.shape}")
            # -------------------------- åŸé€»è¾‘ä¿ç•™ï¼šx_t è®¡ç®— --------------------------
            x_1 = torch.randn_like(x_0).to(self.device)
            t_ = t.unsqueeze(1).unsqueeze(1) # é€‚é… latent ç»´åº¦ (B, 1, 1)
            # print(f"x_0 è®¾å¤‡ï¼š{x_0.device}")
            # print(f"x_1 è®¾å¤‡ï¼š{x_1.device}")
            # print(f"t_ è®¾å¤‡ï¼š{t_.device}")
            x_t = ((1 - t_) * x_0 + t_ * x_1).to(self.flux_pipe.dtype)
            #print(f"x_t:{x_t.dtype}")
            # r=t
            if image_latent_mask is not None:
                x_0 = x_0[:, image_latent_mask[0]]
                x_1 = x_1[:, image_latent_mask[0]]
                x_t = x_t[:, image_latent_mask[0]]
                img_ids = img_ids[image_latent_mask[0]]

            # Prepare conditions
            img_size = (imgs.shape[2], imgs.shape[3])
            #print(f"empty_image:{img_size}")
            condition_empty = Image.new("RGB", img_size, (0, 0, 0))
            condition_latents, uc_latents,condition_ids = [], [], []
            for cond, p_delta, p_scale, latent_mask in zip(
                conditions, position_deltas, position_scales, latent_masks
            ):
                # Prepare conditions
                c_latents, c_ids = encode_images(self.flux_pipe, cond)
                c_latents = c_latents.to(self.device)
                c_ids = c_ids.to(self.device)
                # Scale the position (see OminiConrtol2)
                if p_scale != 1.0:
                    scale_bias = (p_scale - 1.0) / 2
                    c_ids[:, 1:] *= p_scale
                    c_ids[:, 1:] += scale_bias
                # Add position delta (see OminiControl)
                # c_ids[:, 1] += p_delta[0][0]
                # c_ids[:, 2] += p_delta[0][1]
                # if len(p_delta) > 1:
                #     print("Warning: only the first position delta is used.")
                # Append to the list
                if latent_mask is not None:
                    c_latents, c_ids = c_latents[latent_mask], c_ids[latent_mask[0]]
                condition_latents.append(c_latents)
                condition_ids.append(c_ids)
                uc_latents.append(encode_images(self.flux_pipe, condition_empty)[0].expand(batch_size, -1, -1))

            # Prepare guidance
            guidance = (
                torch.ones_like(t).to(self.device)
                if self.transformer.config.guidance_embeds
                else None
            )

        branch_n = 2 + len(conditions)
        group_mask = torch.ones([branch_n, branch_n], dtype=torch.bool).to(self.device)
        # Disable the attention cross different condition branches
        group_mask[2:, 2:] = torch.diag(torch.tensor([1] * len(conditions)))
        # Disable the attention from condition branches to image branch and text branch
        if self.model_config.get("independent_condition", False):
            group_mask[2:, :2] = False

        # -------------------------- Mean Flows æ”¹åŠ¨2: Positional Embeddingï¼ˆt, t-rï¼‰--------------------------
        # è®¡ç®—æ—¶é—´é—´éš” delta_t = t - rï¼ˆè®ºæ–‡Tab.1cæœ€ä¼˜é…ç½®ï¼‰
        delta_t = t - r
        v_t = x_1 - x_0
        # print(f"[{local_rank}] Base v_t mean: {v_t.abs().mean().item():.4f}, {v_t.shape}")
        # print(f"delta_t:{delta_t},{delta_t.dtype},{delta_t.shape}")

        # # -------------------------- Forward Passï¼ˆé€‚é… Mean Flows å¹³å‡é€Ÿåº¦é¢„æµ‹ï¼‰--------------------------
        # # æ¨¡å‹è¾“å‡º u_thetaï¼šé¢„æµ‹å¹³å‡é€Ÿåº¦ u(z_t, r, t)
        # transformer_out = transformer_forward(
        #     self.transformer,
        #     image_features=[x_t, *(condition_latents)],
        #     text_features=[prompt_embeds],
        #     img_ids=[img_ids, *(condition_ids)],
        #     txt_ids=[text_ids],
        #     # ä¼ å…¥åŸå§‹ tï¼ˆç”¨äºæ¨¡å‹å†…éƒ¨è®¡ç®—ï¼‰ï¼Œå¹¶æ·»åŠ  delta_t ä½œä¸ºä½ç½®ç¼–ç 
        #     timesteps=[t, t] + [torch.zeros_like(t)] * len(conditions),
        #     delta_t=[delta_t, delta_t] + [torch.zeros_like(delta_t)] * len(conditions),  # æ‰€æœ‰åˆ†æ”¯å…±äº« delta_t
        #     pooled_projections=[pooled_prompt_embeds] * branch_n,
        #     guidances=[guidance] * branch_n,
        #     adapters=self.adapter_names,
        #     return_dict=False,
        #     group_mask=group_mask,
        # )
        # u_theta = transformer_out[0]  # æ¨¡å‹è¾“å‡ºï¼šå¹³å‡é€Ÿåº¦é¢„æµ‹å€¼

        # def manual_chunked_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None,
        #                              chunk_size=512):
        #     """
        #     æ‰‹åŠ¨å®ç°çš„åˆ†å—æ³¨æ„åŠ›æœºåˆ¶ (Memory Efficient Math Attention)ã€‚
        #     æ”¯æŒ JVPï¼Œä¸”é€šè¿‡åˆ†å—è®¡ç®—é¿å… OOMã€‚
        #
        #     Args:
        #         chunk_size: æ¯æ¬¡å¤„ç†çš„ Query é•¿åº¦ã€‚è¶Šå°è¶Šçœæ˜¾å­˜ï¼Œä½†é€Ÿåº¦ç¨æ…¢ã€‚å»ºè®® 256-1024ã€‚
        #     """
        #     B, H, L, D = query.shape
        #     _, _, S, _ = key.shape
        #
        #     if scale is None:
        #         scale = 1 / math.sqrt(D)
        #
        #     # 1. å‡†å¤‡ Output å®¹å™¨
        #     output = torch.empty_like(query)
        #
        #     # 2. åªæœ‰åœ¨ mask å­˜åœ¨æ—¶æ‰å¤„ç† mask
        #     # attn_mask shape é€šå¸¸æ˜¯ (B, 1, L, S) æˆ– (B, H, L, S)
        #
        #     # 3. åˆ†å—å¾ªç¯ (Slicing)
        #     for i in range(0, L, chunk_size):
        #         end = min(i + chunk_size, L)
        #
        #         # [Batch, Heads, Chunk, Dim]
        #         q_chunk = query[:, :, i:end, :]
        #
        #         # (Q @ K.T) * scale -> [Batch, Heads, Chunk, S]
        #         # ä½¿ç”¨ torch.matmul ä¿è¯ JVP å…¼å®¹æ€§
        #         attn_scores = torch.matmul(q_chunk, key.transpose(-1, -2)) * scale
        #
        #         # å¤„ç† Mask
        #         if attn_mask is not None:
        #             # åˆ‡ç‰‡ Mask: mask[:, :, i:end, :]
        #             mask_chunk = attn_mask[:, :, i:end, :]
        #             attn_scores = attn_scores + mask_chunk
        #
        #         if is_causal:
        #             # å¦‚æœæ˜¯ Causal Maskï¼Œéœ€è¦åŠ¨æ€ç”Ÿæˆ
        #             # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šé€šå¸¸ Flux ä¸ç”¨ is_causal=Trueï¼Œè€Œæ˜¯ä¼ å…¥ attn_mask
        #             # å¦‚æœç¡®å®é‡åˆ° is_causal=Trueï¼Œå»ºè®®ä½¿ç”¨ torch.ones æ„é€ ä¸‹ä¸‰è§’æ©ç å¹¶åˆ‡ç‰‡
        #             pass
        #
        #             # Softmax (åœ¨æœ€åä¸€ä¸ªç»´åº¦ S ä¸Šå½’ä¸€åŒ–)
        #         attn_probs = F.softmax(attn_scores, dim=-1, dtype=torch.float32)
        #
        #         # Dropout (è®­ç»ƒæ—¶é€šå¸¸ä¸º 0ï¼ŒJVP ä¹Ÿä¸å»ºè®®å¼€ Dropout)
        #         if dropout_p > 0.0:
        #             attn_probs = F.dropout(attn_probs, p=dropout_p, training=True)
        #
        #         # (A @ V) -> [Batch, Heads, Chunk, Dim]
        #         o_chunk = torch.matmul(attn_probs, value)
        #
        #         # å†™å…¥ç»“æœ
        #         output[:, :, i:end, :] = o_chunk
        #
        #         # æ˜¾å¼é‡Šæ”¾ä¸´æ—¶æ˜¾å­˜ (è™½ç„¶ Python ä¼šè‡ªåŠ¨å›æ”¶ï¼Œä½†åœ¨é«˜å‹ä¸‹è¿™å¾ˆæœ‰ç”¨)
        #         del q_chunk, attn_scores, attn_probs, o_chunk
        #
        #     return output
        # @contextmanager
        # def temporary_fp32_execution():
        #     """
        #     ç»ˆæ Monkey Patchï¼š
        #     1. åŠ«æŒ Linear/LayerNorm/GroupNorm/Embedding -> è§£å†³ BF16 vs FP32 ç±»å‹ä¸åŒ¹é…ã€‚
        #     2. å…¨å±€åŠ«æŒ scaled_dot_product_attention -> è§£å†³ JVP ä¸æ”¯æŒ FlashAttention çš„é—®é¢˜ã€‚
        #     """
        #     # ================= 1. ä¿å­˜åŸå§‹æ–¹æ³• =================
        #     orig_linear_forward = nn.Linear.forward
        #     orig_layer_norm_forward = nn.LayerNorm.forward
        #     orig_group_norm_forward = nn.GroupNorm.forward
        #     orig_embedding_forward = nn.Embedding.forward
        #
        #     # å…³é”®ï¼šä¿å­˜åŸå§‹çš„ SDPA å‡½æ•°æŒ‡é’ˆ
        #     orig_sdpa = F.scaled_dot_product_attention
        #
        #     # ================= 2. å®šä¹‰ Patch æ–¹æ³• =================
        #
        #     # [å…³é”®] åŠ«æŒ SDPAï¼š
        #     # æ— è®ºåœ¨ checkpoint å†…éƒ¨è¿˜æ˜¯å¤–éƒ¨ï¼Œå¼ºåˆ¶åŒ…è£¹åœ¨ MATH kernel ä¸Šä¸‹æ–‡ä¸­
        #     def new_sdpa(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None, **kwargs):
        #         # å¼ºåˆ¶ä½¿ç”¨åˆ†å—æ³¨æ„åŠ›ï¼Œchunk_size å¯è°ƒï¼ˆå¦‚æœè¿˜OOMï¼Œè°ƒå°è¿™ä¸ªå€¼ï¼Œå¦‚ 256ï¼‰
        #         return manual_chunked_attention(
        #             query, key, value,
        #             attn_mask=attn_mask,
        #             dropout_p=dropout_p,
        #             is_causal=is_causal,
        #             scale=scale,
        #             chunk_size=128  # <--- å…³é”®è°ƒä¼˜å‚æ•°
        #         )
        #
        #     # Linear Patch: åŠ¨æ€è½¬æƒé‡
        #     def new_linear_forward(self, input):
        #         if input.dtype == torch.float32 and self.weight is not None and self.weight.dtype != torch.float32:
        #             weight = self.weight.float()
        #             bias = self.bias.float() if self.bias is not None else None
        #             return F.linear(input, weight, bias)
        #         return orig_linear_forward(self, input)
        #
        #     # LayerNorm Patch
        #     def new_layer_norm_forward(self, input):
        #         if input.dtype == torch.float32 and self.weight is not None and self.weight.dtype != torch.float32:
        #             weight = self.weight.float()
        #             bias = self.bias.float() if self.bias is not None else None
        #             return F.layer_norm(input, self.normalized_shape, weight, bias, self.eps)
        #         return orig_layer_norm_forward(self, input)
        #
        #     # GroupNorm Patch
        #     def new_group_norm_forward(self, input):
        #         if input.dtype == torch.float32 and self.weight is not None and self.weight.dtype != torch.float32:
        #             weight = self.weight.float()
        #             bias = self.bias.float() if self.bias is not None else None
        #             return F.group_norm(input, self.num_groups, weight, bias, self.eps)
        #         return orig_group_norm_forward(self, input)
        #
        #     # Embedding Patch
        #     def new_embedding_forward(self, input):
        #         if self.weight is not None and self.weight.dtype != torch.float32:
        #             return F.embedding(
        #                 input, self.weight.float(), self.padding_idx, self.max_norm,
        #                 self.norm_type, self.scale_grad_by_freq, self.sparse
        #             )
        #         return orig_embedding_forward(self, input)
        #
        #     # ================= 3. åº”ç”¨å…¨å±€ Patch =================
        #     # ä¿®æ”¹ç±»æ–¹æ³•
        #     nn.Linear.forward = new_linear_forward
        #     nn.LayerNorm.forward = new_layer_norm_forward
        #     nn.GroupNorm.forward = new_group_norm_forward
        #     nn.Embedding.forward = new_embedding_forward
        #
        #     # ä¿®æ”¹å‡½æ•°æ¨¡å— (è¿™æ˜¯æœ€é‡è¦çš„ä¸€æ­¥ï¼Œè¦†ç›–å…¨å±€å‘½åç©ºé—´)
        #     F.scaled_dot_product_attention = new_sdpa
        #
        #     try:
        #         yield
        #     finally:
        #         # ================= 4. è¿˜åŸåŸå§‹æ–¹æ³• =================
        #         nn.Linear.forward = orig_linear_forward
        #         nn.LayerNorm.forward = orig_layer_norm_forward
        #         nn.GroupNorm.forward = orig_group_norm_forward
        #         nn.Embedding.forward = orig_embedding_forward
        #         F.scaled_dot_product_attention = orig_sdpa
        # # -------------------------- Mean Flows æ”¹åŠ¨3: æŸå¤±å‡½æ•°ï¼ˆMeanFlow Identityï¼‰--------------------------
        # def compute_jvp_result(self, x_t, r, t):
        #     """
        #     è®¡ç®—å…¬å¼: [u(xt,t,t), 0, 1] . [du/dx, du/dr, du/dt]
        #     å…¨ç¨‹ä½¿ç”¨ Float32 ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
        #     """
        #
        #     # ==========================================
        #     # æ­¥éª¤ 1: å‡†å¤‡ Float32 ç¯å¢ƒ
        #     # ==========================================
        #
        #     # # æå– FP32 æƒé‡ (ä¸å½±å“åŸæ¨¡å‹ï¼Œå ç”¨é¢å¤–æ˜¾å­˜)
        #     # # æ³¨æ„ï¼šå¦‚æœæ˜¾å­˜éå¸¸ç´§å¼ ï¼Œå¯ä»¥åœ¨è¿™é‡ŒæŠŠ tensor æ”¾åœ¨ CPUï¼Œfunctional_call ä¼šè‡ªåŠ¨å¤„ç†(å¯èƒ½ä¼šæ…¢)ï¼Œ
        #     # # æˆ–è€…åªè½¬æ¢å¿…è¦çš„å±‚ã€‚è¿™é‡Œå‡è®¾æ˜¾å­˜è¶³å¤Ÿã€‚
        #     # params_f32 = {k: v.float() for k, v in self.transformer.named_parameters()}
        #     # buffers_f32 = {k: v.float() for k, v in self.transformer.named_buffers()}
        #
        #     # å‡†å¤‡è¾“å…¥æ•°æ®ä¸º FP32
        #     xt_f32 = x_t.float()
        #     r_f32 = r.float()
        #     t_f32 = t.float()
        #
        #     # è¿˜éœ€è¦ç¡®ä¿è¾…åŠ©å˜é‡ (condition_latents ç­‰) ä¹Ÿæ˜¯ FP32
        #     # è¿™é‡Œå‡è®¾ä½ å¯ä»¥è®¿é—®è¿™äº›å˜é‡ï¼Œä½ éœ€è¦æ ¹æ®å®é™…æƒ…å†µå°†å®ƒä»¬è½¬ä¸º float
        #     # cond_latents_f32 = [c.float() for c in condition_latents]
        #     # prompt_embeds_f32 = prompt_embeds.float()
        #     # ... å…¶ä»–æ‰€æœ‰ä¼ å…¥ transformer_forward çš„ Tensor éƒ½éœ€è¦æ˜¯ float32
        #
        #     # ==========================================
        #     # æ­¥éª¤ 2: å®šä¹‰çº¯å‡½æ•° (Pure Function)
        #     # ==========================================
        #
        #     # å®šä¹‰ä¸€ä¸ªä»£ç†ç±»ï¼Œç”¨äºæ¬ºéª— transformer_forward
        #     # å½“ transformer_forward è°ƒç”¨ model(...) æ—¶ï¼Œå®é™…ä¸Šæ˜¯åœ¨æ‰§è¡Œ functional_call
        #     # class StatelessModel:
        #     #     def __call__(self_, *args, **kwargs):
        #     #         # å…³é”®ç‚¹ï¼šè¿™é‡Œå¼ºè¡Œä½¿ç”¨ params_f32 è¿›è¡Œå‰å‘ä¼ æ’­
        #     #         return functional_call(self.transformer, (params_f32, buffers_f32), args, kwargs)
        #     #
        #     #     # å¦‚æœ transformer_forward è®¿é—®äº† config ç­‰å±æ€§ï¼Œä»£ç†ç»™åŸæ¨¡å‹
        #     #     def __getattr__(self_, name):
        #     #         return getattr(self.transformer, name)
        #     #
        #     # stateless_model = StatelessModel()
        #     #
        #     # é‡å†™ä¸€ä»½é€»è¾‘ï¼Œå»æ‰ .to(bfloat16)ï¼Œå¹¶ä½¿ç”¨ stateless_model
        def u_theta_pure_f32(z_in, r_in, t_in,use_kernel=True):
            # è¿™é‡Œçš„è¾“å…¥å·²ç»æ˜¯ float32 äº†ï¼Œåƒä¸‡ä¸è¦å† cast æˆ bf16
            delta_t = t_in - r_in

            # é€»è¾‘å¤ç”¨ (å‡è®¾ cond=True)
             # ç¡®ä¿è¿™äº›ä¹Ÿæ˜¯ float32
            # print(f"z_in: {z_in.dtype}, r_in: {r_in.dtype}, t_in: {t_in.dtype}")
            # print(f"condition_latents: {condition_latents[0].dtype}, prompt_embeds: {prompt_embeds.dtype}, pooled_projections: {pooled_prompt_embeds.dtype}")
            # è°ƒç”¨ transformer_forwardï¼Œä½†åœ¨ç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥æˆ‘ä»¬çš„ä»£ç†æ¨¡å‹
            with torch.autocast("cuda", enabled=True):
                out = transformer_forward(
                    self.transformer,  # <--- æ³¨å…¥ç‚¹ï¼šä½¿ç”¨æºå¸¦ FP32 æƒé‡çš„ä»£ç†
                    image_features=[z_in, condition_latents[0]],  # z_in æ˜¯ JVP çš„å˜é‡
                    text_features=[prompt_embeds],  # ç¡®ä¿æ˜¯ float32
                    img_ids=[img_ids, *condition_ids],
                    txt_ids=[text_ids],
                    timesteps=[t_in, t_in] + [torch.zeros_like(t_in)] * len(conditions),
                    delta_t=[delta_t, delta_t] + [torch.zeros_like(delta_t)] * len(conditions),
                    pooled_projections=[pooled_prompt_embeds] * branch_n,  # ç¡®ä¿æ˜¯ float32
                    guidances=[guidance] * branch_n,
                    adapters=self.adapter_names,
                    return_dict=False,
                    use_kernel=use_kernel,
                    group_mask=group_mask,
                )[0]

            # ç¡®ä¿è¾“å‡ºæ˜¯ float32 (è™½ç„¶ functional_call ç”¨ float32 æƒé‡è·‘å‡ºæ¥é€šå¸¸å°±æ˜¯ float32)
            return out
        #
        #
        #
        #     # ==========================================
        #     # æ­¥éª¤ 3: è®¡ç®— Tangent Vector (v_x, 0, 1)
        #     # ==========================================
        #
        #     # è®¡ç®—å‘é‡çš„ç¬¬ä¸€é¡¹ u(x_t, t, t)ã€‚
        #     # ä½¿ç”¨åˆšå®šä¹‰çš„çº¯å‡½æ•°è®¡ç®—ï¼Œç¡®ä¿ç²¾åº¦ä¸€è‡´ã€‚
        #     with temporary_fp32_execution():
        #         v_x=v_t.float()
        #
        #         v_r = torch.zeros_like(r_f32)
        #         v_t_ = torch.ones_like(t_f32)
        #
        #         # ==========================================
        #         # æ­¥éª¤ 4: æ‰§è¡Œ JVP
        #         # ==========================================
        #
        #         primals = (xt_f32, r_f32, t_f32)
        #         tangents = (v_x, v_r, v_t_)
        #         if hasattr(self.transformer, 'gradient_checkpointing') and not self.transformer.gradient_checkpointing:
        #             self.transformer.enable_gradient_checkpointing()
        #         # u_val æ˜¯å‡½æ•°å€¼ï¼Œjvp_val æ˜¯ä½ éœ€è¦çš„ç»“æœ
        #         u_val, jvp_val = jvp(u_theta_pure_f32, primals, tangents)
        #
        #     # å¦‚æœåç»­æµç¨‹éœ€è¦ bf16ï¼Œå¯ä»¥åœ¨è¿™é‡Œè½¬å›ï¼Œå¦åˆ™è¿”å› float32
        #     return u_val.bfloat16(),jvp_val.bfloat16()
        #
        #
        # u_val, dudt_ = compute_jvp_result(self, x_t, r, t)
        # print(f"[{local_rank}] Base u_val mean: {u_val.abs().mean().item():.4f}, {u_val.shape}")
        # print(f"[{local_rank}] Base dudt_ mean: {dudt_.abs().mean().item():.4f}, {dudt_.shape}")
        # 2. è®¡ç®— dudt = æ€»å¯¼æ•°ï¼ˆä½¿ç”¨ JVP é«˜æ•ˆè®¡ç®—ï¼Œè®ºæ–‡4.1å…¬å¼8ï¼‰
        # å®šä¹‰è¾…åŠ©å‡½æ•°ï¼šè¾“å…¥ (z, t, delta_t)ï¼Œè¾“å‡º u_theta
        def u_theta_cfg_fn(z, r_in, t_in, cond=True, use_kernel=False):
            """
            è®¡ç®—å¸¦CFGçš„u_thetaï¼šåŒºåˆ†æ¡ä»¶/æ— æ¡ä»¶è¾“å‡º
            :param z: å«å™ªæ ·æœ¬ z_t
            :param r_in: èµ·å§‹æ—¶é—´ r
            :param t_in: å½“å‰æ—¶é—´ t
            :param cond: True=ç±»åˆ«æ¡ä»¶è¾“å‡ºï¼ŒFalse=ç±»åˆ«æ— æ¡ä»¶è¾“å‡º
            :return: u_theta^{cfg}(z_t, t, t | c) æˆ– u_theta^{cfg}(z_t, t, t)
            """
            # z = z.to(torch.bfloat16)
            # r_in = r_in.to(torch.bfloat16)
            # t_in = t_in.to(torch.bfloat16)
            delta_t = t_in - r_in
            # æ¡ä»¶å¼€å…³ï¼šcond=Trueæ—¶ä¼ å…¥ç±»åˆ«æ¡ä»¶ï¼ŒFalseæ—¶æ¸…ç©º
            _condition_latents = condition_latents if cond else uc_latents

            with torch.autocast("cuda", enabled=True):
                out = transformer_forward(
                    self.transformer,  # <--- æ³¨å…¥ç‚¹ï¼šä½¿ç”¨æºå¸¦ FP32 æƒé‡çš„ä»£ç†
                    image_features=[z, condition_latents[0]],  # z_in æ˜¯ JVP çš„å˜é‡
                    text_features=[prompt_embeds],  # ç¡®ä¿æ˜¯ float32
                    img_ids=[img_ids, *condition_ids],
                    txt_ids=[text_ids],
                    timesteps=[t_in, t_in] + [torch.zeros_like(t_in)] * len(conditions),
                    delta_t=[delta_t, delta_t] + [torch.zeros_like(delta_t)] * len(conditions),
                    pooled_projections=[pooled_prompt_embeds] * branch_n,  # ç¡®ä¿æ˜¯ float32
                    guidances=[guidance] * branch_n,
                    adapters=self.adapter_names,
                    return_dict=False,
                    use_kernel=use_kernel,
                    group_mask=group_mask,
                )[0]
            return out

        # è®¡ç®— u_theta^{cfg}(z_t, t, t | c)ï¼šç±»åˆ«æ¡ä»¶è¾“å‡ºï¼ˆr=tï¼Œæ—¶é—´é—´éš”ä¸º0ï¼‰
        # u_cfg_cond = u_theta_cfg_fn(x_t, t, t, cond=True)
        # #print(f"[{local_rank}] Base u_cfg_cond mean: {u_cfg_cond.abs().mean().item():.4f}, {u_cfg_cond.shape}")
        # # è®¡ç®— u_theta^{cfg}(z_t, t, t)ï¼šç±»åˆ«æ— æ¡ä»¶è¾“å‡ºï¼ˆr=tï¼Œæ—¶é—´é—´éš”ä¸º0ï¼‰
        # u_cfg_uncond = u_theta_cfg_fn(x_t, t, t, cond=False)
        # #print(f"[{local_rank}] Base u_cfg_uncond mean: {u_cfg_uncond.abs().mean().item():.4f}, {u_cfg_uncond.shape}")
        #
        # # 3. æŒ‰è®ºæ–‡å…¬å¼è®¡ç®— v_t
        # # v_t = Ï‰*(Îµ - x) + Îº*u_cfg_cond + (1-Ï‰-Îº)*u_cfg_uncond
        # v_t = self.omega * (x_1 - x_0) + self.kappa * u_cfg_cond + (1 - self.omega - self.kappa) * u_cfg_uncond

        #print(f"[{local_rank}] Base v_t mean: {v_t.abs().mean().item():.4f}, {v_t.shape}")
        #print(f"v_t:{v_t.dtype}")
        # 2. æ­£ç¡®è°ƒç”¨ JVPï¼šfn ä¸ºå¯è°ƒç”¨å‡½æ•°ï¼Œè¾“å…¥/åˆ‡çº¿å‘é‡ä¸¥æ ¼å¯¹é½ (z, r, t)
        # æ³¨æ„ï¼šJVP çš„ fn å¿…é¡»æ˜¯ "è¾“å…¥å‚æ•°â†’è¾“å‡º" çš„å¯è°ƒç”¨å¯¹è±¡ï¼Œä¸èƒ½ç›´æ¥ä¼ å‡½æ•°è°ƒç”¨ç»“æœ

        u_out, dudt_ = torch.func.jvp(
            u_theta_pure_f32,  # å°è£…ä¸ºå¯è°ƒç”¨ lambda
            (x_t, r, t),  # è¾“å…¥ï¼š(z_t=x_t, r=èµ·å§‹æ—¶é—´, t=å½“å‰æ—¶é—´)
            (v_t, torch.zeros_like(r).to(self.flux_pipe.dtype), torch.ones_like(t).to(self.flux_pipe.dtype)) # è®ºæ–‡å…¬å¼8çš„ (v, 0, 1)
        )
        # dudt_per_batch_mean = dudt_.flatten(1).mean(dim=1).abs()
        #
        # # æ­¥éª¤2ï¼šæ‰“å°ç»“æœï¼ˆä¸æ–¹æ¡ˆ 1 ä¸€è‡´ï¼Œä¸¤ç§æ‰“å°æ–¹å¼ä»»é€‰ï¼‰
        # per_batch_str = ", ".join([f"{x:.4f}" for x in dudt_per_batch_mean.tolist()])
        # print(f"[{local_rank}]use_kernel=true dudt_ : [{per_batch_str}], original shape: {dudt_.shape}")
        # u_out, dudt = torch.func.jvp(
        #     u_theta_cfg_fn,  # å°è£…ä¸ºå¯è°ƒç”¨ lambda
        #     (x_t, r, t),  # è¾“å…¥ï¼š(z_t=x_t, r=èµ·å§‹æ—¶é—´, t=å½“å‰æ—¶é—´)
        #     (v_t, torch.zeros_like(r).to(self.flux_pipe.dtype), torch.ones_like(t).to(self.flux_pipe.dtype))
        #     # è®ºæ–‡å…¬å¼8çš„ (v, 0, 1)
        # )
        # dudt_per_batch_mean = dudt.flatten(1).mean(dim=1).abs()
        #
        # # æ­¥éª¤2ï¼šæ‰“å°ç»“æœï¼ˆä¸æ–¹æ¡ˆ 1 ä¸€è‡´ï¼Œä¸¤ç§æ‰“å°æ–¹å¼ä»»é€‰ï¼‰
        # per_batch_str = ", ".join([f"{x:.4f}" for x in dudt_per_batch_mean.tolist()])
        # print(f"[{local_rank}]use_kernel=false dudt : [{per_batch_str}], original shape: {dudt_.shape}")
        # print(f"[{local_rank}] Base u_out mean: {u_out_.abs().mean().item():.4f}, {u_out_.shape}")
        # epsilon = torch.tensor(1e-2, device=x_t.device, dtype=x_t.dtype)
        #
        # # 2. è®¡ç®—å½“å‰ç‚¹çš„è¾“å‡º (åŸºå‡†ç‚¹)
        u_out_ = u_theta_pure_f32(x_t, r, t, use_kernel=False)
        # print(f"[{local_rank}] Base u_out_ mean: {u_out_.abs().mean().item():.4f}, {u_out_.shape}")
        # #print(f"u_out:{u_out.dtype}")
        # # 3. å‡†å¤‡æ‰°åŠ¨åçš„è¾“å…¥
        # # å› ä¸º epsilon æ˜¯ BFloat16ï¼Œè¿™é‡Œçš„åŠ æ³•å’Œä¹˜æ³•ç»“æœä¼šä¿æŒ BFloat16
        # x_t_perturbed = x_t + epsilon * v_t
        # r_perturbed = r
        # t_perturbed = t + epsilon
        #
        # # 4. è®¡ç®—æ‰°åŠ¨åçš„è¾“å‡º
        # u_perturbed = u_theta_cfg_fn(x_t_perturbed, r_perturbed, t_perturbed, cond=True)
        #
        # # 5. è®¡ç®—å…¨å¯¼æ•°
        # # æ‰€æœ‰æ“ä½œæ•°éƒ½æ˜¯ BFloat16ï¼Œé™¤æ³•ç»“æœä¹Ÿæ˜¯ BFloat16
        # dudt_= (u_perturbed - u_out_) / epsilon
        #print(f"[{local_rank}] Base dudt_ mean: {dudt_.abs().mean().item():.4f}, {dudt_.shape}")

        # æ— éœ€å…³é—­ Flash Attentionï¼Œç›´æ¥è¿è¡Œ
        # 1. ä¸­å¿ƒå·®åˆ†æ­¥é•¿
        # eps_val = 1e-2
        # epsilon = torch.tensor(eps_val, device=x_t.device, dtype=x_t.dtype)
        #
        # # 2. èŠ‚çœæ˜¾å­˜æŠ€å·§ï¼šä½¿ç”¨ no_grad è®¡ç®—ä¸¤ä¸ªæ‰°åŠ¨ç‚¹
        # with torch.no_grad():
        #     # t + eps
        #     u_plus = u_theta_cfg_fn(x_t + epsilon * v_t, r, t + epsilon, cond=True)
        #     # t - eps
        #     u_minus = u_theta_cfg_fn(x_t - epsilon * v_t, r, t - epsilon, cond=True)
        #
        #     # 3. è½¬ float32 è®¡ç®—é«˜ç²¾åº¦å·®åˆ†
        #     dudt = (u_plus.to(torch.float32) - u_minus.to(torch.float32)) / (2 * eps_val)
        #     dudt = dudt.to(dtype=v_t.dtype)
        #     dudt_per_batch_mean = dudt.flatten(1).mean(dim=1).abs()
        #
        #     # æ­¥éª¤2ï¼šæ‰“å°ç»“æœï¼ˆä¸æ–¹æ¡ˆ 1 ä¸€è‡´ï¼Œä¸¤ç§æ‰“å°æ–¹å¼ä»»é€‰ï¼‰
        #     per_batch_str = ", ".join([f"{x:.4f}" for x in dudt_per_batch_mean.tolist()])
        #     print(f"[{local_rank}] Base dudt per batch mean: [{per_batch_str}], original shape: {dudt.shape}")

        # 4. æ­£å¸¸å‰å‘ä¼ æ’­ (å¸¦æ¢¯åº¦)
        # print(f"[{local_rank}] Base u_out mean: {u_out_.abs().mean().item():.4f}, {u_out_.shape}")
        # 3. è®¡ç®—ç›®æ ‡å¹³å‡é€Ÿåº¦ u_tgtï¼ˆè®ºæ–‡4.1å…¬å¼10ï¼‰
        delta_t_expanded = delta_t.unsqueeze(1).unsqueeze(1)  # é€‚é… latent ç»´åº¦
        # print(f"[{local_rank}] Base delta_t_expanded mean: {delta_t_expanded.abs().mean().item():.4f}, {delta_t_expanded.shape}")
        u_tgt = v_t - delta_t_expanded * dudt_
        # print(f"[{local_rank}] Base u_tgt mean: {u_tgt.abs().mean().item():.4f}, {u_tgt.shape}")
        #print(f"u_tgt:{u_tgt.dtype}, {u_tgt.shape}")
        # 4. è®¡ç®— MSE æŸå¤±ï¼ˆè®ºæ–‡4.1å…¬å¼9ï¼‰ï¼Œå¯¹ u_tgt æ–½åŠ  stop-gradient
        def adaptive_weighted_loss(pred, target, c=1e-3, p=0.5):
            """
            è‡ªé€‚åº”åŠ æƒ L2 æŸå¤±å‡½æ•°

            Args:
                pred: æ¨¡å‹é¢„æµ‹å€¼ (u_out)
                target: ç›®æ ‡å€¼ (u_tgt)ï¼Œä¼šè‡ªåŠ¨ detach
                c: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼Œé»˜è®¤ 1e-3
                p: æƒé‡æŒ‡æ•°ï¼Œp = 1 - Î³ï¼Œé»˜è®¤ 1
            Returns:
                åŠ æƒæŸå¤±æ ‡é‡
            """
            # ç¡®ä¿å¸¸æ•°ç±»å‹ä¸€è‡´
            c = torch.tensor(c, device=pred.device, dtype=pred.dtype)

            # è®¡ç®—å›å½’è¯¯å·®
            delta = pred - target.detach()

            # L2 å¹³æ–¹è¯¯å·®
            l2_squared = delta ** 2

            # è‡ªé€‚åº”æƒé‡ï¼ˆå¸¦ stop gradientï¼‰
            weight = (1.0 / (l2_squared + c) ** p).detach()
            #print(f"[{local_rank}] weight mean: {weight.abs().mean().item():.4f}, {weight.shape}")
            # åŠ æƒæŸå¤±
            loss = (weight * l2_squared).mean()

            return loss

        # ä½¿ç”¨æ–¹å¼
        step_loss = adaptive_weighted_loss(u_out_, u_tgt, c=1e-3, p=1)
        # step_loss = F.mse_loss(u_out, u_tgt.detach(), reduction="mean")

        # -------------------------- åŸé€»è¾‘ä¿ç•™ï¼šæ—¥å¿—å’Œè¿”å› --------------------------
        self.last_t = t.mean().item()
        self.last_r = r.mean().item()  # æ–°å¢ï¼šè®°å½• r çš„å‡å€¼
        self.last_delta_t = delta_t.mean().item()  # æ–°å¢ï¼šè®°å½•æ—¶é—´é—´éš”å‡å€¼

        self.log_loss = (
            step_loss.item()
            if not hasattr(self, "log_loss")
            else self.log_loss * 0.95 + step_loss.item() * 0.05
        )
        return step_loss

    def generate_a_sample(self):
        raise NotImplementedError("Generate a sample not implemented.")


class TrainingCallback(L.Callback):
    def __init__(self, run_name, training_config: dict = {}, test_function=None):
        self.run_name, self.training_config = run_name, training_config

        self.print_every_n_steps = training_config.get("print_every_n_steps", 10)
        self.save_interval = training_config.get("save_interval", 1000)
        self.sample_interval = training_config.get("sample_interval", 1000)
        self.save_path = training_config.get("save_path", "./output")

        self.wandb_config = training_config.get("wandb", None)
        self.use_wandb = (
            wandb is not None and os.environ.get("WANDB_API_KEY") is not None
        )

        self.total_steps = 0
        self.test_function = test_function

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        gradient_size = 0
        max_gradient_size = 0
        count = 0
        for _, param in pl_module.named_parameters():
            if param.grad is not None:
                gradient_size += param.grad.norm(2).item()
                max_gradient_size = max(max_gradient_size, param.grad.norm(2).item())
                count += 1
        if count > 0:
            gradient_size /= count

        self.total_steps += 1

        # Print training progress every n steps
        if self.use_wandb:
            report_dict = {
                "steps": batch_idx,
                "steps": self.total_steps,
                "epoch": trainer.current_epoch,
                "gradient_size": gradient_size,
            }
            loss_value = outputs["loss"].item() * trainer.accumulate_grad_batches
            report_dict["loss"] = loss_value
            report_dict["t"] = pl_module.last_t
            wandb.log(report_dict)

        if self.total_steps % self.print_every_n_steps == 0:
            print(
                f"Epoch: {trainer.current_epoch}, Steps: {self.total_steps}, Batch: {batch_idx}, Loss: {pl_module.log_loss:.4f}, Gradient size: {gradient_size:.4f}, Max gradient size: {max_gradient_size:.4f}"
            )

        # Save LoRA weights at specified intervals
        if self.total_steps % self.save_interval == 0:
            print(
                f"Epoch: {trainer.current_epoch}, Steps: {self.total_steps} - Saving LoRA weights"
            )
            pl_module.save_lora(
                f"{self.save_path}/{self.run_name}/ckpt/{self.total_steps}"
            )
            pl_module.save_custom_embed_weights(
                f"{self.save_path}/{self.run_name}/ckpt/{self.total_steps}"
            )

        # Generate and save a sample image at specified intervals
        if self.total_steps % self.sample_interval == 0 and self.test_function:
            print(
                f"Epoch: {trainer.current_epoch}, Steps: {self.total_steps} - Generating a sample"
            )
            pl_module.eval()
            self.test_function(
                pl_module,
                f"{self.save_path}/{self.run_name}/output",
                f"lora_{self.total_steps}",
            )
            pl_module.train()


def train(dataset, trainable_model, config, test_function):
    # Initialize
    is_main_process, rank = get_rank() == 0, get_rank()
    torch.cuda.set_device(rank)
    # config = get_config()

    training_config = config["train"]
    run_name = time.strftime("%Y%m%d-%H%M%S")

    # Initialize WanDB
    wandb_config = training_config.get("wandb", None)
    if wandb_config is not None and is_main_process:
        init_wandb(wandb_config, run_name)

    # print("Rank:", rank)
    if is_main_process:
        print("Config:", config)

    # Initialize dataloader
    print("Dataset length:", len(dataset))
    train_loader = DataLoader(
        dataset,
        batch_size=training_config.get("batch_size", 1),
        shuffle=True,
        num_workers=training_config["dataloader_workers"],
    )

    # Callbacks for testing and saving checkpoints
    if is_main_process:
        callbacks = [TrainingCallback(run_name, training_config, test_function)]

    # Initialize trainer
    trainer = L.Trainer(
        # accelerator="cuda",  # ä½¿ç”¨ CUDA
        # devices=1,  # ä» Lightning è§’åº¦çœ‹æ˜¯ 1 ä¸ª"è®¾å¤‡"ï¼ˆä½†æ¨¡å‹å†…éƒ¨è·¨å¤šå¡ï¼‰
        # strategy="auto",
        accumulate_grad_batches=training_config["accumulate_grad_batches"],
        callbacks=callbacks if is_main_process else [],
        enable_checkpointing=False,
        enable_progress_bar=False,
        logger=False,
        max_steps=training_config.get("max_steps", -1),
        max_epochs=training_config.get("max_epochs", -1),
        gradient_clip_val=training_config.get("gradient_clip_val", 0.5),
    )

    setattr(trainer, "training_config", training_config)
    setattr(trainable_model, "training_config", training_config)

    # Save the training config
    save_path = training_config.get("save_path", "./output")
    if is_main_process:
        os.makedirs(f"{save_path}/{run_name}")
        with open(f"{save_path}/{run_name}/config.yaml", "w") as f:
            yaml.dump(config, f)

    # Start training
    trainer.fit(trainable_model, train_loader)
